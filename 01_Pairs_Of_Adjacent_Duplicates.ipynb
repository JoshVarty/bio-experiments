{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairs Of Adjacent Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the first problem mostly working, let's try another.\n",
    "\n",
    "\n",
    "Given a sequence of numbers, find and mark adjacent duplicates. For simplicity, we'll only have one duplicate in each input.\n",
    "\n",
    "```\n",
    "input  = 0 3 5 9 3 3 5 2 5\n",
    "output = 0 0 0 0 1 1 0 0 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from radam import RAdam\n",
    "from transformers import BertConfig, BertModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import train, get_accuracy, get_output_for_example, plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_examples: int, seq_len: int, vocab_size: int):\n",
    "    inputs = np.random.randint(0, vocab_size, size=(num_examples, seq_len))\n",
    "    outputs = np.zeros_like(inputs)\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "        \n",
    "        # choose number of duplicates to introduce\n",
    "        num_duplicates = 2\n",
    "        \n",
    "        # choose random location to introduce our duplicates\n",
    "        location = np.random.randint(0, len(inputs[i]) - num_duplicates)\n",
    "        \n",
    "        # Choose what number we'd like to repeat\n",
    "        number_to_repeat = np.random.randint(0, vocab_size)\n",
    "        \n",
    "        inputs[i][location:location + num_duplicates] = number_to_repeat\n",
    "        \n",
    "        # Mark location where duplicates exist\n",
    "        outputs[i][location:location + num_duplicates] = 1\n",
    "        \n",
    "\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[313, 896, 158, 158, 322, 134, 241, 354, 379, 733]]),\n",
       " array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're lazy so we're going to just use a large vocabulary \n",
    "# instead of manually removing accidental duplicates\n",
    "generate_data(num_examples=1, seq_len=10, vocab_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "   \n",
    "    def __init__(self, num_examples, sequence_length, vocab_size):\n",
    "        self.items, self.labels = generate_data(num_examples, sequence_length, vocab_size)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = torch.Tensor(self.items[idx]).long()\n",
    "        y = torch.Tensor(self.labels[idx]).long()\n",
    "        return x.cuda(), y.cuda()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around a BERT model that predicts a single binary output for each input token\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Untrained BERT Model\n",
    "        config = BertConfig(vocab_size_or_config_json_file=vocab_size)\n",
    "        self.bert_model = BertModel(config)\n",
    "        self.linear = torch.nn.Linear(in_features=768, out_features=2)\n",
    "        \n",
    "    def forward(self, x):    \n",
    "        out, _ = self.bert_model(x)\n",
    "        out = self.linear(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_EXAMPLES = 128\n",
    "VAL_EXAMPLES = 120\n",
    "SEQ_LENGTH = 10\n",
    "VOCAB_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "\n",
    "train_ds = ToyDataset(num_examples=TRN_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "valid_ds = ToyDataset(num_examples=VAL_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel(VOCAB_SIZE)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We use RAdam to avoid having to use warmup\n",
    "# If we use regular Adam, this usually won't converge for long sequences\n",
    "optimizer = RAdam(model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.549916684627533\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.79348963\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.5504586100578308\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.8000001\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.5850911140441895\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.7893229\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.49532175064086914\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.79453135\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.46877628564834595\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.7984375\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.4457995295524597\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.76744795\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.4453398585319519\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.7963542\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.4241681694984436\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.7757813\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.4092821180820465\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.7945312\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.39121687412261963\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.7710938\n",
      "Epoch:\t 10 \tStep:\t 0 \tLoss:\t 0.3617556095123291\n",
      "Epoch:\t 10 \t\t\tValid Accuracy\t 0.7861979\n",
      "Epoch:\t 11 \tStep:\t 0 \tLoss:\t 0.3348979949951172\n",
      "Epoch:\t 11 \t\t\tValid Accuracy\t 0.7601563\n",
      "Epoch:\t 12 \tStep:\t 0 \tLoss:\t 0.28562724590301514\n",
      "Epoch:\t 12 \t\t\tValid Accuracy\t 0.7526042\n",
      "Epoch:\t 13 \tStep:\t 0 \tLoss:\t 0.27779704332351685\n",
      "Epoch:\t 13 \t\t\tValid Accuracy\t 0.7622396\n",
      "Epoch:\t 14 \tStep:\t 0 \tLoss:\t 0.2340778112411499\n",
      "Epoch:\t 14 \t\t\tValid Accuracy\t 0.7401042\n",
      "Epoch:\t 15 \tStep:\t 0 \tLoss:\t 0.20675548911094666\n",
      "Epoch:\t 15 \t\t\tValid Accuracy\t 0.72526044\n",
      "Epoch:\t 16 \tStep:\t 0 \tLoss:\t 0.15954186022281647\n",
      "Epoch:\t 16 \t\t\tValid Accuracy\t 0.7132813\n",
      "Epoch:\t 17 \tStep:\t 0 \tLoss:\t 0.13569150865077972\n",
      "Epoch:\t 17 \t\t\tValid Accuracy\t 0.7341146\n",
      "Epoch:\t 18 \tStep:\t 0 \tLoss:\t 0.10551704466342926\n",
      "Epoch:\t 18 \t\t\tValid Accuracy\t 0.75703126\n",
      "Epoch:\t 19 \tStep:\t 0 \tLoss:\t 0.0885644182562828\n",
      "Epoch:\t 19 \t\t\tValid Accuracy\t 0.7328125\n",
      "Epoch:\t 20 \tStep:\t 0 \tLoss:\t 0.05721881985664368\n",
      "Epoch:\t 20 \t\t\tValid Accuracy\t 0.74609375\n",
      "Epoch:\t 21 \tStep:\t 0 \tLoss:\t 0.05751236528158188\n",
      "Epoch:\t 21 \t\t\tValid Accuracy\t 0.7302084\n",
      "Epoch:\t 22 \tStep:\t 0 \tLoss:\t 0.04424375668168068\n",
      "Epoch:\t 22 \t\t\tValid Accuracy\t 0.74609375\n",
      "Epoch:\t 23 \tStep:\t 0 \tLoss:\t 0.04364718496799469\n",
      "Epoch:\t 23 \t\t\tValid Accuracy\t 0.73906255\n",
      "Epoch:\t 24 \tStep:\t 0 \tLoss:\t 0.04555123671889305\n",
      "Epoch:\t 24 \t\t\tValid Accuracy\t 0.74661463\n",
      "Epoch:\t 25 \tStep:\t 0 \tLoss:\t 0.02534933015704155\n",
      "Epoch:\t 25 \t\t\tValid Accuracy\t 0.74947923\n",
      "Epoch:\t 26 \tStep:\t 0 \tLoss:\t 0.03155488520860672\n",
      "Epoch:\t 26 \t\t\tValid Accuracy\t 0.73333335\n",
      "Epoch:\t 27 \tStep:\t 0 \tLoss:\t 0.018499713391065598\n",
      "Epoch:\t 27 \t\t\tValid Accuracy\t 0.7455729\n",
      "Epoch:\t 28 \tStep:\t 0 \tLoss:\t 0.03423496335744858\n",
      "Epoch:\t 28 \t\t\tValid Accuracy\t 0.7416667\n",
      "Epoch:\t 29 \tStep:\t 0 \tLoss:\t 0.011795727536082268\n",
      "Epoch:\t 29 \t\t\tValid Accuracy\t 0.7497396\n",
      "Epoch:\t 30 \tStep:\t 0 \tLoss:\t 0.024978643283247948\n",
      "Epoch:\t 30 \t\t\tValid Accuracy\t 0.74895835\n",
      "Epoch:\t 31 \tStep:\t 0 \tLoss:\t 0.024497222155332565\n",
      "Epoch:\t 31 \t\t\tValid Accuracy\t 0.734375\n",
      "Epoch:\t 32 \tStep:\t 0 \tLoss:\t 0.025470327585935593\n",
      "Epoch:\t 32 \t\t\tValid Accuracy\t 0.7377604\n",
      "Epoch:\t 33 \tStep:\t 0 \tLoss:\t 0.01284765638411045\n",
      "Epoch:\t 33 \t\t\tValid Accuracy\t 0.73802084\n",
      "Epoch:\t 34 \tStep:\t 0 \tLoss:\t 0.003491396550089121\n",
      "Epoch:\t 34 \t\t\tValid Accuracy\t 0.7320313\n",
      "Epoch:\t 35 \tStep:\t 0 \tLoss:\t 0.00599034084007144\n",
      "Epoch:\t 35 \t\t\tValid Accuracy\t 0.7473959\n",
      "Epoch:\t 36 \tStep:\t 0 \tLoss:\t 0.007704878691583872\n",
      "Epoch:\t 36 \t\t\tValid Accuracy\t 0.7419271\n",
      "Epoch:\t 37 \tStep:\t 0 \tLoss:\t 0.008442127145826817\n",
      "Epoch:\t 37 \t\t\tValid Accuracy\t 0.74583334\n",
      "Epoch:\t 38 \tStep:\t 0 \tLoss:\t 0.007864253595471382\n",
      "Epoch:\t 38 \t\t\tValid Accuracy\t 0.7481771\n",
      "Epoch:\t 39 \tStep:\t 0 \tLoss:\t 0.006281844340264797\n",
      "Epoch:\t 39 \t\t\tValid Accuracy\t 0.7166667\n",
      "Epoch:\t 40 \tStep:\t 0 \tLoss:\t 0.005251122638583183\n",
      "Epoch:\t 40 \t\t\tValid Accuracy\t 0.74713546\n",
      "Epoch:\t 41 \tStep:\t 0 \tLoss:\t 0.006551565136760473\n",
      "Epoch:\t 41 \t\t\tValid Accuracy\t 0.7445312\n",
      "Epoch:\t 42 \tStep:\t 0 \tLoss:\t 0.0045783016830682755\n",
      "Epoch:\t 42 \t\t\tValid Accuracy\t 0.7109375\n",
      "Epoch:\t 43 \tStep:\t 0 \tLoss:\t 0.017162365838885307\n",
      "Epoch:\t 43 \t\t\tValid Accuracy\t 0.7494792\n",
      "Epoch:\t 44 \tStep:\t 0 \tLoss:\t 0.00337233766913414\n",
      "Epoch:\t 44 \t\t\tValid Accuracy\t 0.74895835\n",
      "Epoch:\t 45 \tStep:\t 0 \tLoss:\t 0.0025804585311561823\n",
      "Epoch:\t 45 \t\t\tValid Accuracy\t 0.73932296\n",
      "Epoch:\t 46 \tStep:\t 0 \tLoss:\t 0.012474337592720985\n",
      "Epoch:\t 46 \t\t\tValid Accuracy\t 0.7348958\n",
      "Epoch:\t 47 \tStep:\t 0 \tLoss:\t 0.007703407201915979\n",
      "Epoch:\t 47 \t\t\tValid Accuracy\t 0.73932296\n",
      "Epoch:\t 48 \tStep:\t 0 \tLoss:\t 0.006756164133548737\n",
      "Epoch:\t 48 \t\t\tValid Accuracy\t 0.7322917\n",
      "Epoch:\t 49 \tStep:\t 0 \tLoss:\t 0.008888394571840763\n",
      "Epoch:\t 49 \t\t\tValid Accuracy\t 0.73567706\n"
     ]
    }
   ],
   "source": [
    "trn_loss, val_loss, val_acc = train(model, train_dl, valid_dl, loss_fn, optimizer, num_epochs=50, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(trn_loss, val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we're getting low training loss, we've clearly overfit our dataset as validation loss has exploded. Additionally we're getting about 80% validation accuracy at best which is as much as we could expect when predicting all `0`. (80% of each example is `0`).\n",
    "\n",
    "Let's try adding ten times as much training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ten times as much data as before\n",
    "TRN_EXAMPLES = 1280\n",
    "VAL_EXAMPLES = 1200\n",
    "# Other parameters are kept the same\n",
    "SEQ_LENGTH = 10\n",
    "VOCAB_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "\n",
    "train_ds = ToyDataset(num_examples=TRN_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "valid_ds = ToyDataset(num_examples=VAL_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel(VOCAB_SIZE)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We use RAdam to avoid having to use warmup\n",
    "# If we use regular Adam, this usually won't converge for long sequences\n",
    "optimizer = RAdam(model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.6584423184394836\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.7999999\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.47321218252182007\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.799671\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.46234917640686035\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.79654604\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.4707900583744049\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.7849507\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.46136125922203064\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.7602796\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.4570659101009369\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.7592928\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.4252626895904541\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.7473685\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.3852044939994812\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.71422696\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.3179878294467926\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.7283717\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.3127977252006531\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.7277138\n",
      "Epoch:\t 10 \tStep:\t 0 \tLoss:\t 0.27355897426605225\n",
      "Epoch:\t 10 \t\t\tValid Accuracy\t 0.7144737\n",
      "Epoch:\t 11 \tStep:\t 0 \tLoss:\t 0.3094567060470581\n",
      "Epoch:\t 11 \t\t\tValid Accuracy\t 0.69958884\n",
      "Epoch:\t 12 \tStep:\t 0 \tLoss:\t 0.3079269528388977\n",
      "Epoch:\t 12 \t\t\tValid Accuracy\t 0.7074836\n",
      "Epoch:\t 13 \tStep:\t 0 \tLoss:\t 0.07088611274957657\n",
      "Epoch:\t 13 \t\t\tValid Accuracy\t 0.70312506\n",
      "Epoch:\t 14 \tStep:\t 0 \tLoss:\t 0.19449356198310852\n",
      "Epoch:\t 14 \t\t\tValid Accuracy\t 0.6936679\n",
      "Epoch:\t 15 \tStep:\t 0 \tLoss:\t 0.05683515593409538\n",
      "Epoch:\t 15 \t\t\tValid Accuracy\t 0.71825653\n",
      "Epoch:\t 16 \tStep:\t 0 \tLoss:\t 0.03331498056650162\n",
      "Epoch:\t 16 \t\t\tValid Accuracy\t 0.7140625\n",
      "Epoch:\t 17 \tStep:\t 0 \tLoss:\t 0.04822627827525139\n",
      "Epoch:\t 17 \t\t\tValid Accuracy\t 0.7040296\n",
      "Epoch:\t 18 \tStep:\t 0 \tLoss:\t 0.06758962571620941\n",
      "Epoch:\t 18 \t\t\tValid Accuracy\t 0.70518094\n",
      "Epoch:\t 19 \tStep:\t 0 \tLoss:\t 0.0016121391672641039\n",
      "Epoch:\t 19 \t\t\tValid Accuracy\t 0.71546054\n",
      "Epoch:\t 20 \tStep:\t 0 \tLoss:\t 0.0018842887366190553\n",
      "Epoch:\t 20 \t\t\tValid Accuracy\t 0.6995066\n",
      "Epoch:\t 21 \tStep:\t 0 \tLoss:\t 0.002552510006353259\n",
      "Epoch:\t 21 \t\t\tValid Accuracy\t 0.709704\n",
      "Epoch:\t 22 \tStep:\t 0 \tLoss:\t 0.0007689855992794037\n",
      "Epoch:\t 22 \t\t\tValid Accuracy\t 0.6959704\n",
      "Epoch:\t 23 \tStep:\t 0 \tLoss:\t 0.004803050309419632\n",
      "Epoch:\t 23 \t\t\tValid Accuracy\t 0.7089638\n",
      "Epoch:\t 24 \tStep:\t 0 \tLoss:\t 0.028785893693566322\n",
      "Epoch:\t 24 \t\t\tValid Accuracy\t 0.7078125\n",
      "Epoch:\t 25 \tStep:\t 0 \tLoss:\t 0.018506282940506935\n",
      "Epoch:\t 25 \t\t\tValid Accuracy\t 0.69621706\n",
      "Epoch:\t 26 \tStep:\t 0 \tLoss:\t 0.016691094264388084\n",
      "Epoch:\t 26 \t\t\tValid Accuracy\t 0.7\n",
      "Epoch:\t 27 \tStep:\t 0 \tLoss:\t 0.004191185813397169\n",
      "Epoch:\t 27 \t\t\tValid Accuracy\t 0.69958884\n",
      "Epoch:\t 28 \tStep:\t 0 \tLoss:\t 0.0023452609311789274\n",
      "Epoch:\t 28 \t\t\tValid Accuracy\t 0.70616776\n",
      "Epoch:\t 29 \tStep:\t 0 \tLoss:\t 0.0016533511225134134\n",
      "Epoch:\t 29 \t\t\tValid Accuracy\t 0.6962171\n",
      "Epoch:\t 30 \tStep:\t 0 \tLoss:\t 0.00017099939577747136\n",
      "Epoch:\t 30 \t\t\tValid Accuracy\t 0.7089638\n",
      "Epoch:\t 31 \tStep:\t 0 \tLoss:\t 0.02461734227836132\n",
      "Epoch:\t 31 \t\t\tValid Accuracy\t 0.65213823\n",
      "Epoch:\t 32 \tStep:\t 0 \tLoss:\t 0.0006826480966992676\n",
      "Epoch:\t 32 \t\t\tValid Accuracy\t 0.7038652\n",
      "Epoch:\t 33 \tStep:\t 0 \tLoss:\t 0.0006904072943143547\n",
      "Epoch:\t 33 \t\t\tValid Accuracy\t 0.68634874\n",
      "Epoch:\t 34 \tStep:\t 0 \tLoss:\t 0.0001438349427189678\n",
      "Epoch:\t 34 \t\t\tValid Accuracy\t 0.7080592\n",
      "Epoch:\t 35 \tStep:\t 0 \tLoss:\t 0.004132300615310669\n",
      "Epoch:\t 35 \t\t\tValid Accuracy\t 0.7212171\n",
      "Epoch:\t 36 \tStep:\t 0 \tLoss:\t 0.010380055755376816\n",
      "Epoch:\t 36 \t\t\tValid Accuracy\t 0.72113496\n",
      "Epoch:\t 37 \tStep:\t 0 \tLoss:\t 0.02231663651764393\n",
      "Epoch:\t 37 \t\t\tValid Accuracy\t 0.70871717\n",
      "Epoch:\t 38 \tStep:\t 0 \tLoss:\t 0.0004740908625535667\n",
      "Epoch:\t 38 \t\t\tValid Accuracy\t 0.7260691\n",
      "Epoch:\t 39 \tStep:\t 0 \tLoss:\t 0.003514312906190753\n",
      "Epoch:\t 39 \t\t\tValid Accuracy\t 0.69185853\n",
      "Epoch:\t 40 \tStep:\t 0 \tLoss:\t 0.0004791170358657837\n",
      "Epoch:\t 40 \t\t\tValid Accuracy\t 0.6993421\n",
      "Epoch:\t 41 \tStep:\t 0 \tLoss:\t 0.000207686418434605\n",
      "Epoch:\t 41 \t\t\tValid Accuracy\t 0.7138158\n",
      "Epoch:\t 42 \tStep:\t 0 \tLoss:\t 0.0036154245026409626\n",
      "Epoch:\t 42 \t\t\tValid Accuracy\t 0.71513164\n",
      "Epoch:\t 43 \tStep:\t 0 \tLoss:\t 5.094632433610968e-05\n",
      "Epoch:\t 43 \t\t\tValid Accuracy\t 0.7067434\n",
      "Epoch:\t 44 \tStep:\t 0 \tLoss:\t 0.007296179886907339\n",
      "Epoch:\t 44 \t\t\tValid Accuracy\t 0.7155428\n",
      "Epoch:\t 45 \tStep:\t 0 \tLoss:\t 0.0028048758395016193\n",
      "Epoch:\t 45 \t\t\tValid Accuracy\t 0.7170231\n",
      "Epoch:\t 46 \tStep:\t 0 \tLoss:\t 0.005915137007832527\n",
      "Epoch:\t 46 \t\t\tValid Accuracy\t 0.67648035\n",
      "Epoch:\t 47 \tStep:\t 0 \tLoss:\t 0.06360194087028503\n",
      "Epoch:\t 47 \t\t\tValid Accuracy\t 0.72565794\n",
      "Epoch:\t 48 \tStep:\t 0 \tLoss:\t 0.003988820128142834\n",
      "Epoch:\t 48 \t\t\tValid Accuracy\t 0.7130757\n",
      "Epoch:\t 49 \tStep:\t 0 \tLoss:\t 0.009330295026302338\n",
      "Epoch:\t 49 \t\t\tValid Accuracy\t 0.70657897\n"
     ]
    }
   ],
   "source": [
    "trn_loss, val_loss, val_acc = train(model, train_dl, valid_dl, loss_fn, optimizer, num_epochs=50, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEJCAYAAACE39xMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydd3xb1fn/30fDe++ROHZ2nL0TAgFCmWEEKFA2YRdKaWlpKd/SSVso9EcbSqEpu+xdRliBhDASIAmJs+Nsz3jvpXF+fxxJcRwPSZYsyT7v10uvK0t3nCtLn/vc5zxDSCnRaDQaTehjCPQANBqNRuMbtKBrNBrNIEELukaj0QwStKBrNBrNIEELukaj0QwSTIE6cEpKiszNzQ3U4TUajSYk2bBhQ5WUMrW79wIm6Lm5uaxfvz5Qh9doNJqQRAhxsKf3tMtFo9FoBgla0DUajWaQoAVdo9FoBgla0DUajWaQoAVdo9FoBgla0DUajWaQoAVdo9FoBgla0DUajf+wtsPGZ8FuD/RIhgRa0DUajf/Y/SG8fRsc+irQIxkSaEHXaDT+o6FELav3BHYcQwQt6BqNxn84Bb1mX2DHMUTQgq7RaPxHQ5laVu8N7DiGCFrQNRqN/2goVUttoQ8IfQq6EGK4EGKVEGK7EGKbEOL2btYRQohlQog9QogCIcQM/wxXo9GEFI1OQd+vI10GAHcsdCvwMyllPjAPuFUIkd9lnTOBMY7HjcCjPh2lRqMJPaRULpfweLC2QmNZoEc06OlT0KWUZVLKjY7njcAOILvLaucBz0rFOiBBCJHp89FqNJrQoaUGbO0w4jj1d432o/sbj3zoQohcYDrwdZe3soGiTn8Xc6zoI4S4UQixXgixvrKy0rORajSa0MLpbsk9Xi21H93vuC3oQogY4HXgJ1LKBm8OJqVcLqWcJaWclZrabQcljUYzWHBOiA6bDcYwHekyALgl6EIIM0rMn5dSvtHNKiXA8E5/D3O8ptFohipOQY8fBol52kIfANyJchHAE8AOKeX/62G1t4GrHNEu84B6KaWeAdFohjINpSAMEJMOSSO1oA8A7jSJXgBcCWwRQmxyvHY3kAMgpXwMWAGcBewBWoClvh+qRqMJKRpLlZgbTZA8CvatUqGLBp3+4i/6FHQp5ReA6GMdCdzqq0FpNJpBQEMpxDqC3ZJGgrVNhS7GHxMvcTQvXgrD58DxP/X/GAcZ+lKpCX72rFQhcJrQoqEM4rLU86SRatlX6GJ7I+x6HwpX+ndsgxQt6JrgpnwLPHchPP49lW2oCR0aS48IevIotewr0qV0EyChutCvQxusaEHXBDe7P1TLlmp44jTHD36AkBI+/i0c3j5wxxwsdDRDW/0RQY/LVqGLfU2MlmxQy6bD0OZVdPSQRgu6Jrgp/BiypsN1H4EpHJ5eDHs+GZhjN5TCl3+Hjc8MzPEGE84qi7EOQTcY3QtdLN145Lm20j1GC7omeGmpgeJvYMxpkDoOrvsYEnPhhYth80v+P37dIbV0Wo0a93HWQY/rVAEkeZQbFvp3kD5JPa/STTE8RQu6JnjZ+ylIO4w5Xf0dlwlLV0DOfHjzJvjiIeUW8RdOQS8rAGuH/44zGHEW4orrFNHijEXvqepicxXUH4KJ54MwagvdC7Sga4KXwo8gKkW5XJxExMMVr8Ok78PK38G6f/nv+E5Bt7VDhfaje4TTQo/tZKG7QhdLu9+mxOFuyZkHiSOgSgu6p2hB1wQndpsKVxz9vWMTUUzhcMF/YMQCWP+k/6z0uoNqIg+028VTGsogIgHCoo685ox06cntUrJBZZZmToPkMboPqRdoQdcEJyUbVWTLmFO7f99ggEkXqB995S7/jKHuEGROhcikI9ajxj0aOoUsOnHGovcUuli6EVLGQXgMpIxR6+mmGB6hBV0TnBR+pKy1UYt6XmfcYrXc8Y5/xlB3EBJGQPbMo6MvNH3T2I2gxw0DY3j3yUVSqotmtqPZWfJo1RSjodj/Yx1EaEHXBCeFH8HwuRCV1PM6cZmqNOtOPwi63Qb1xZCQowS9YofKYtS4R0PZ0f5zUHdVSXndJ4jVF0FL1ZH5kpQxahnsfvQDX8ArV4O1PdAjAbSga4KRxnIo29Szu6UzE86Bss1HJjB9NoYysFsdgj4DkOo4mr6xWVRiUFcLHZTbpTuXi9Ol5bLQHYIezH700k3wwiWw/S2V0RwEaEHXBB97HHU8xpzW97rjz1bLHe/6dgzOC0RCDmQ5RGag/ejW9tD0ITcdBmTPgl7bTcPo0o1qAtoZgx6TBuFxwWuhV++F57+vJugBDm8L7HgcaEHXBB+FH6nbdeePuzeSR0FaPuz0l6CPgJhUJewDFenS0Qyf/gnuy4GvQ7DfurOxRWwPgt5d6GLJRvX/dgqkEMqPHoyx6I2H4bkLVI7E0vfBHKVcckGAFnRNcGGzwN5Vyt0ieq3afIQJ58ChtSoxxVc4BT1+mFpmzfC/hS4lFLwCD8+CNX8FxJFaNqGEU9C7s9C7K9Jltyv3hdPd4iRlTPBli7bVq2JxTZVw2asqgzl1PFRoC12jOZZD66C94Uh2qDuMP1tZS7tW+G4cdQchJgPMEerv7Jkqi7HJT83NizfAE6fCGzdAbDpc+xHMuBKK14PN6p9j+oveBD2pm1j06kLoaDzi2nKSPEZFuXQ0+2ecnmJpgxcvg8odcMl/YdhM9Xp6ftAUcNOCrgkuCj8CgxlGnuj+NhmTlUvEl+GLdYfUPp1kO368vg5ftFnhrVvh8UXqmOf9C67/FHLmqoxJSzMcDo4JN7dpLAVTBEQmHvteXPaxoYtdJ0SdpIxWy2BoLm23wRvXw8EvYMljMPqUI++lTVQROk0VgRufAy3omuCi8GMYcRyEx7q/jRAw/hzYt9p3JVe7CnrmVBUX72s/+oHPYdNzMPsGuG0DTL/8SGbs8HlqeWidb4/pb5whi925zJyhi9WdLPTSjRAWAyljj17XFekSBH70r5Ypg+GM+2DKRUe/l56vlkEwMaoFXRM81B1St7PuRLd0ZcI5YOuAPR/3fxydY9CdhMcoX6mv/ejOUMiT7z72IhafrcZwaK1vj+lvGkqPLsrVlaQuVRdLNqh0f4Px6PWSRwEiOPzoZQWq/O+8Hx77XtpEtQyCej9a0DX+wdoOllbPtin8SC29EfThcyA61TduF2cMeuKIo1/PnqHEx5e1Y8o2Q3xOzwlUOfOVhe7PqpK+prH06LK5XUnKOxK6aO1QMdzZ049dzxwJ8cP7ttA7mtVEuj8/o9ZaiEru/r2YVPXdCwI/uhZ0jX944wZ49jzPfmSFjnrnzixBTzAYYdxZah+WNs+370ztQbXsbKGD8qO31kDtgf7tvzPlBZA5pef3c+apuO7aEGm/J2X3WaKdSR6lQhcbSlR0iK3j2AlRJymj+45F/+yv8N8lsOYB78fdF6213c8JOEnLD4pIFy3oGv9QsROKvob9n7m3vqUN9n2mrHN3wxW7MuFc6Ghy/5g90TkGvTO+nhhtb1QTfplTe14n1PzoLTWq3HBfLhdQbpeeJkSdOKsu9mYY7HxPJSWt+hOsfcS7cfdFX4KePlF95+02/xzfTbSga/xDU7lafvkP99Yv/EgVYxrtRrp/T+QtVNmF/XW7dI1Bd5KWr6I3fOVHL98KSMjoxUJPHa9qwIeKH727TkVdcVZdrNmrLo5RycdePJ2kjFEX6cby7t+vKlQumVP/oC7oH94NG572evg90lrXt4VubfXt3ZsXaEHX+B5Lq0rAiE5TXYf6qoFisyrrKmkkjDrZ++OawpSFv2tF/2K36w4pl4Eza9GJ0azE11eRLuUFatmby8VgUFb6oa99c0x/012noq64QhcdFnrWjJ7vypKdoYs9uF12vqeW48+GC59QBsE7P4GCV70bf3fYbdBe34eFHhyRLlrQNb6n6bBaLrgdwmLhy2W9r7/pOajcCd/7nRLN/jDhbFVHvagfLoq6g8f6z51kz1QXKF8k+5QVqMm03vzNoPzoVbugubr/x/Q3rrT/Xs7JGbpYvkX933tyt0DfVRd3rVAX2YTh6oJ+yX8h93jVotAp9v2lrV4texP01PGACHikixZ0zRHqS+Dr5f2PFmh0CHrqOJh1DWx7o+db0fYmWPVnVSp3wrn9Oy4oC80Y3r9iXV1j0DuTPQMsLUqI+kvZZiVGfc0Z5MxXy6IQsNIbSlW8fkx67+sljYL9n6sM354mREHVgzFHdV91sakSir6B8YuPvGaOhEtfVGV4X71G3SH2l9ZatexN0MOi1YS+ttA1QUHVHnjydHj/zv53AHL6z2PSYd4tquFvT5NVa/+pLPrT7vV+MrQz4TEw8qQjIZCeYrMqP3BvFjr03+1ibVcx9725W5xkTVeTfqHgR28sVf93o6n39ZJHgnRMIPZmoRsMKiqmOwt99weAVNFNnQmPhSteU92PXrys+/rrnuCOoINjYjSwRbq0oGvUrf9TZxyZeHJObHmL00KPzVD1PKZcAhv/e2zxrMbDyh2Tf56KI/cVw+eoCTfnD9ETOtdB746kkWqSsr+RLhU71HF6i3BxYo5QVmwoRLp013quO5wTo/HDVanc3kge070PfdcKtX3G5GPfi0yEC/+jJioPftX3eHrDJegJva+Xlq++d57mX/gQLehDnUPr4OmzlZvi0pfUa86JLW9pLFNWeVSK+nvBj9UP65v/HL3e6j+rGORTftu/43XFFV74nefbdq6D3h1CqP3310J3ThT3FuHSmZy56nwCKBZu0VcMuhNn6GJWNwlFXUkZo/4vnbsCdbSoZKJxZ/YyoTpG3dn01z3mtoWer1xI/upx6wZa0IcyhSvh2SUq0+3aDyDvBPV6Q2nv2/VF02FldTlrkqSOU7fF3/z7SOW8ip2w8VmYfd2Rkqq+wikS3oQX9hSDftT+Z6iswI4Wz/fvpLxATRgn5rm3fs58sFuCv1m1uxa6c7Jz2Oy+100eo4Syc7mAfauUkdDV3dIZo0lt21+BdVfQg6AEgBb0ocq2N+HFH6hMvKUfOKIEwlXURb9dLuXK3dKZBberH8Z3z6m/V/5WFWRa+Iv+Has7IhOUBeiVoB8ExLEx6J3Jnqn8v/1pO1bmyBA1uPkTHD5XLb2J3mmrh/VP+b/7UUezCu9zR9DjsuCq/8Hs6/te11l1sbMffecKCI9XES29kTpORQj1B6egR/Thckkaqe50AzgxqgV9KLLjXXjtWiVMV7+rLHQncVk+stC7CHrOPBVP/dU/1a3y7g/ghDsguof6GP0le6Z3fu6eYtCP2rezJZ2Xbhe7DQ5vdd/dAqrWS+p47/zo6x6Fd38CB7/0fFtPaHC46rrrVNQdI0+CsKi+1+sai263qe/PmFP7DnNNHa9KOfTnbqq1ViWs9TXRazSpC4i20DUDyqbnIW4YXPnmsRM9sT4Q9MZy1aShKwtuV00iXrlKHX/uzf07Tm9kz1C+fE/PpbeQRSexGWr8Jeu9G1v1HhX66E6ES2dyHAlGnljazi5IoNwU/sSVJeqmoLtLeKy6yDqrLhZ/q+qPj+/F3eIkdRwg+1eCt7W27wlRJ+kTA1qkSwv6UKSsQEWCdGcd9ddCt1nUj62rhQ4w9gwVStbeAKfco2KG/YW3jZ17SyrqTM5cOLjWu5j9MkeGqCcWOig/enu9Cnd0l9KNKvLCYPJNTHZvuLJEfSzocHR/0Z3vqSYo7pSJSB2nlv3xo/eV9t+ZtHwVtttS4/3x+oEW9KFGS41q69VdqBeoH2NrjffRFM4s0e4sdIMBzvgLzLoWJl/s3f7dJXOKirTxxO1is6rkKncEfcRxKubam9odZZuUr9UpNu6S4yzU5UE8esGrKtJj7s2qb6c/s02dFro7US6ekjJG+dClVOGKeSdARFzf2yWNUt+D/kS69FWYqzMBLgGgBX2o0Vf9EGcNDm+tdGcMencWOqjWXWc/5P5koLeYI9WPyxMLvbFUTXa6JegL1NIbv3R5gRqbp2UOEkaoz9VdP7rNCltfh7Gnw8TzAelft0tDmZo4dMcv7inJY6CtTp179Z7eo1s6YwpTUVT9stA9EPQAR7poQR9qOCMzerrdd1bJ81bQnVmiXaNcAkHWDGWhu+sW6SsGvTOp41WVQE+TVqRULhdP3S2g4q1z5rkv6Ps/g+YKdTeUNV2J7V5/CrqbIYve4AxzdFbvHHem+9umjhs4Cz02Q62rLXTNgFBWoCY+o1O6f7/fFnoQCXr2TBWy1zl+uTc8EXQhlE/bUwu97pCyNN3JEO2OnPlQX6Ra5PXFlldVaN+Y01QDkJEnKT+6vzr7NPpR0J2RLrvfV59db2GlXUkdr9L/OycmuYuUngm6EMpK1xa6ZkDoq0OO0//Z6K2FfhgQqnRuoMn2cGK07hB9xqB3ZsQC5UOv9yBu3+Xy8lbQ3Wx40dGi6sLnn6NKBwCMWqT+r/7KZHQ3S9QbEnLUvAPAuMW9r9uV1PHKlVa91/PjtjeqbfuKQe9Mer6jtIOf4/67oU9BF0I8KYSoEEJs7eH9k4QQ9UKITY7Hb3w/TI1PsLRC1e7eb/fDY1Stkv5Y6NEpfcfsDgSpE8AU6X68uDsx6J3JdfrRPXC7lBWoaoRp+e5v05n0SSohq69j7n5fNYboPPnsrDXvj2gXm0VdzHurg94fDMYj9V/cCVfsjCvSxQu3i7tZop1Jy1effX2R58frJ+5Y6E8DZ/SxzudSymmOxx/6PyyNXzi8XaVQ9xTh4iQuux8+9G6SigKF0aQsYXcjXeoOHdsYujfSJ6mEE0/cLmWbIWWs9xOHRpOaWN70Qu+WdsGr6uLUOZMyIUdNLvpD0JsOA7L3TkX9JXOKEvX0SZ5tlzwaEN7dmXgj6OmBmxjtU9CllGuAwARVanxLuaMgVF8JLbGZ3qf/N5Z1H7IYKLJnKKvYZul73Vo3Y9CdGIzKBeKJhV5e4L27xcmZf1X1t1+7tvuG2C01sOdjmHShGmNnRi2CA19450/uDacB4C8LHeCsB1SZCk/LLJsjVa3ygbLQU8erZQAmRn3lQ58vhNgshHhfCDGxp5WEEDcKIdYLIdZXVlb66NAatynfoibJeis8Bf1LLmo8HBwTok6yZqgiTn3Vqe6rDnpPjDhO1QppcuP73FShLnjeRLh0JjYDzn9MlQ/46NfHvr/tTVWad0o3sf6jFqnPw9eleN3pVNRfIuK9NxZSx3tnobfVqaUngh4RB/E5wWmhu8FGYISUcirwMPBWTytKKZdLKWdJKWelpqb2tJrGX5QVKHdLXxZOXLYSH3es2s7YbSpMLlhcLnBkYrQvt0tDifsx6J0Z4XBpHHLDSi/rIwfAE8acCvN/BN/+59juTFteVRm53V04co9XWZa+dru4LHQ/Rbn0l9RxKn7d09aB3ljooCZGA1ACoN+CLqVskFI2OZ6vAMxCiB5i4jQBw25Tt4DuiElcFiB77rTeE81VykcfTBZ60kgVodBXpIsnIYudyZyqWqS543Yp97AGel+c8lvInAb/u/VIGGPdIZVJOuWi7i/c4THKTeRrQd+3WkU2eSp8A0XqeFV+uNbD7kXuNrfoSlq+KlVg7fBsu37Sb0EXQmQIob45Qog5jn2GQDfbIUb1HnWr3deEKHgfi9659VywIIRKqvGXoJvCVE1vdyZGywqUu8tTcejt2N9/UrlXXr9BWZ9bHN3uJ1/U83ajTla+/KYK34yjei8UfgizlvqmjaA/8DbSpbVWRUp5WncofaL6v1Tt9my7fuJO2OKLwFpgnBCiWAhxnRDiZiGEs1Te94GtQojNwDLgB1L6K3NB4zWeFIRyZYt6ODHaufVcMJE9U/kzeyuh6oxBj/MgYcXJiAVQvrXvlndlm33jbulM8ihY/P+Uy2fNAyq6ZfhcNQnYE6MWqeW+1b4ZwzfLlRtn1nW+2Z8/SBmrlt4Iujd3HZnT1NLfBdG64E6Uy6VSykwppVlKOUxK+YSU8jEp5WOO9/8ppZwopZwqpZwnpexnAz+NXygvUEWa3CkI5fSDemqhO6vtBZOFDsqP3ldDirpD6rxNYZ7vP3cBIFVp255oq1e3+/2NcOmOqZfA1Evhs/tVJcberHOAjKkQmeQbsWlrgO+eh0kXBFd0U1fCY9REpacTo55UWuxMymg1v/LNcs/99v1AZ4oOFcoLIG2CewWhIhKUX9jT3qJNQWqhu0rp9pJg5E4d9J7Inqkulr25XcodeXkZfhB0gLMeVPMFBhNMvKD3dQ0G5XbxRRmATc9DRyPMval/+xkIUsd6aaF76SKbf4tKLtrxtnfbe4EW9KGApwWhhHCELnrqcilX1oy7mZYDRVymql/TW6RLfwTdHKlEvTdB3/S8yhDNmubdMfoiPAaueB0ufdm9LlCjFqkLcH9C6+x2+PrfMGzOkcbcwUzqeFWC125zfxtvXS6g6v8njYR1//Juey/Qgj4UaChVNc49ud2PzfRiUjSIskS7kj2j54lRb2PQOzPiOFVvvL3p2Pe2vaUE/fifqubZ/iIpD8Z8z711nX70/rhdCj9SbqR5fuw85UtSx4G17cgEuDv0x0I3GGHuD1WHpaJvvNuHp4cckKNoAouzIJQ7ES5OvEn/76n1XDCQNV117ulu4tIVg+5B2n9XRixQ+yju8sOtL4F3blfHP+lX3u/f18RlqVo3/RH0rx9Vdz4TzvXduPyJM4PTEz+6tz50J9MuUwlRax/xfh8eoAV9KFC+BRCe1cCIy1I+dE8qxgW1he5wCZR+d/Tr1XvhvTvUc2eJVm8YPkd1xukcj263w5s3ga0DLnzC84YW/mbUIjXejmaoK4J9n8H6J1X26StXw5bXet62YoeKkplzffCdV094GuliaVWhvv0R9PAYmHmN8qPXHvR+P24SBCXxNH6nbLMKbwuPcX+buCwVR9tc6Z7VLWXwW+ig3C6jFinXyOcPKsvJGA6n/elIaVpvCI9VLq0Dnfzoax+GA5/DuQ+rzz/YGLUI1j0C9+Wo/7UTY5iaGN/+lioPfMLPjo0v//oxMEXAzKUDOuR+EZmgXInuWuitXqT9d8ecm9T37JvlcPqf+revPtCCPhQoL/B80sqVXFTinki31KhMPH/W8ugPkQmqv2TJBhWr/fE96g5k6mXwvd/55kI04jj1o7W0KSvwkz/ChHNg+pX937c/yFuoeo0aw9TknfMRl6UmDv93K3z6R+V6O+uBI4W+Wmpg88uqVkxUUmDPwVM86V7kbdp/V+KzIX8JbHgGTvyle71QvUQL+mCntU5NAnlqSXVuReesh9IbwZgl2pXsmbDlFdVkOHMaXPyscpX4itzjYe0/VTXDD+6C6FQ4Z1nwZk+awuDM+7t/z2CE8/+txP3Lvyt32oWPq4iejc8qV8TcEJkM7UzqePjuOXVH2df/xVeCDiqEcetr6tjzb+n//npA+9AHO85kGk8zFD1N/w+m1nM9MeEclUF5zjK4YZVvxRwcLhsBb96oSi2c/1joWbCdMRjg1N/DmQ/AzvfgmXNVuYBv/qOs+/QeC6sGLyljVfMJd0JyfSno2TNV+8CvH/UsbNJDtKAPdso9SPnvTFSKSud2NxbdmVQUzBZ6/rlw+2aYebUSK18TmahErqUajrsNRp7o+2MEgrk3wsXPqLmYR+ZAQ3FoWufQKdLFDbeLLwUdYN4t6m5557t9r+slWtAHO+VbVOSJp/HPBoNyu7ibLRoKFvpAMPUHMPJkWNRNnfJQJv88uOp/ylWRmKeSZkIRT0IXnYLuST/R3hi/WIXG+jGEUQv6YKesj6bQveFJLHrTYQiLVZ10hjLH3QZXvRV82bK+YMR8uPUbWLri2E5IoUJ0srr7dNdCF0YVweQLDEaY90Mo+hqK1/tmn10P4Ze9aoIDS5vqpuNJQlFnPGlF11imrfOhQGx68DaxcBd3uxe1OZKKfDmpPf0K1Yd280u+22cndJTLYKZyh4ov9rahQlyWighxJyLA0XrOYrFQXFxMW1s3vS41A0ZERATDhg3DbA6RpJ+BJHWcijjp63vdnzouPREeC9d+6F7VUy/Qgj6Y6W/Ls7hsVfuitbbvaI2mcsieRXFxMbGxseTm5iKCNVxvkCOlpLq6muLiYvLy8gI9nOAjdbwqZ9xU0Xv+gT8EHVR7Oj+hXS6DmfItyq+dkOvd9u7WRZfSZaG3tbWRnJysxTyACCFITk7Wd0k9kepmCQB/Cbof0YI+WLHbVIJLxmTvQ/TcFfT2BpVo4ghZ1GIeePT/oBfcDV3Ugq4JGjY8rXzos/pRa8Ml6H1MjAZr6zmNpjti0tXEZPWe3tfrb6XFAKAFfTDSVAmf/B5yT+i7HVlvxKSrpgx9WehNwRODXldXx7/+5V1DgbPOOou6ujq31//d737Hgw8+6NWxNAFECFUsrTdBt1nUnacWdE3A+fge1RB58d/6F3JlNCtR70vQnUlFQVA6tzdBt1p77+24YsUKEhJ8lESiCW6SR/cu6G31aqkFXRNQDnwBm19UCS6+CI2Ky4JGNwU9CErn3nXXXezdu5dp06Zx5513snr1ak444QTOPfdc8vNVdMGSJUuYOXMmEydOZPny5a5tc3Nzqaqq4sCBA0yYMIEbbriBiRMnctppp9Ha2trrcTdt2sS8efOYMmUK559/PrW1Kstw2bJl5OfnM2XKFH7wgx8A8NlnnzFt2jSmTZvG9OnTaWxs9NOnoemR5NGqBrylh4ljX6f9DxA6bHEwYe2A936mupsvvNM3+4zN7NvX2HQYTJHKL8kR8f/9O9vYXtrgm3E4yM+K47fn9FwU6r777mPr1q1s2rQJgNWrV7Nx40a2bt3qCuF78sknSUpKorW1ldmzZ3PhhReSnHx0H87CwkJefPFF/vOf/3DxxRfz+uuvc8UVV/R43KuuuoqHH36YE088kd/85jf8/ve/5+9//zv33Xcf+/fvJzw83OXOefDBB3nkkUdYsGABTU1NRERE9Pdj0XhK8mhAqhZ6aROOfd9VCz207ti0hT6YWPcvNXN/1l8hLMo3+3Qn/d/Z2CJIIyvmzJlzVDz2smXLmDp1KvPmzaOoqIjCwsJjtsnLy2PaNNXQeebMmRw4cKDH/dfX11NXV8eJJ6piXFdffTVr1qwBYMqUKVx++eU899xzmEzKfmBOLo8AACAASURBVFqwYAF33HEHy5Yto66uzvW6ZgBxdqfqyVjRFromoNQVwWf3w7izYNyZvttvXJaaHGpr6Lkwfw+t53qzpAeS6Ogj9WVWr17NypUrWbt2LVFRUZx00kndxmuHhx+pxWI0Gvt0ufTEe++9x5o1a3jnnXf405/+xJYtW7jrrrtYvHgxK1asYMGCBXz44YeMHz/eq/1rvMTZQWqQCbq20AcLH9ylEnx6aljgLc666L1VXWwsD4oIF4DY2NhefdL19fUkJiYSFRXFzp07WbduXb+PGR8fT2JiIp9//jkA//3vfznxxBOx2+0UFRVx8sknc//991NfX09TUxN79+5l8uTJ/PKXv2T27Nns3OlmBx2N7wiPVUZI1eASdG2hDwZ2faBqLJ/yW0jI8e2+OycX9TTJ2nQYRp/i2+N6SXJyMgsWLGDSpEmceeaZLF68+Kj3zzjjDB577DEmTJjAuHHjmDevH31EO/HMM89w880309LSwsiRI3nqqaew2WxcccUV1NfXI6Xkxz/+MQkJCdxzzz2sWrUKg8HAxIkTOfNMH95Radynt0gXV+nc+IEbjw8QUsqAHHjWrFly/Xr/lJAcUtjt8PB01ej45i9UWzFfUrMPlk2H8/4F0y8/9v2OZvhzlrqYnHAHO3bsYMKEbiaZNAOO/l/0wds/Vp2YfrH32PdW/AIKXoK7Dg38uPpACLFBSjmru/e0yyXUqd2vOrPP+6HvxRwgto/0f93YQhOqJI+Glqoj1nhnQjDtH7Sghz7OFnNZ0/yzf3MERCX3nP4fCq3nNJrucEW67Dv2PS3omoBQVgAGE6T5ryQncVnaQtcMPnoLXdSCrgkI5QWqepw/W57F9iLoTgs9NtN/x9do/EFirqpVpAVdEzSUFXjfkchdekv/bywHY1hIfvk1QxxTmGra3J2gt9X5rjn0AKIFPZRpLIfmCu87ErlLXDa0VHdf96Kx3FGVMTizRDWaXukudNFu1xa6JgA4W8wNhIUO3VvpTeUhPyEaExPj0euaQUTyaKjeq5LynHQ0grRrQdcMMOWb1TJjkn+P4xT0XR9A8XqVXddUqYqBOVrPaTQhSfIosDQfmdyHkM0SBS3ooU1ZgZrY8Xc2W/JoQMCHv4LHT4F/zoQHR8O9qaorUhAJ+l133cUjjzzi+tvZhKKpqYlTTjmFGTNmMHnyZP73v/+5vU8pJXfeeSeTJk1i8uTJvPzyywCUlZWxcOFCpk2bxqRJk/j888+x2Wxcc801rnUfeughn5+jxod0F+kSwoKuU/9DmfIBmBAFSBgOP9miIl3a6h2POrVsb4Spl3a/3ft3qUbVviRjMpx5X49vX3LJJfzkJz/h1ltvBeCVV17hww8/JCIigjfffJO4uDiqqqqYN28e5557rlu9N9944w02bdrE5s2bqaqqYvbs2SxcuJAXXniB008/nf/7v//DZrPR0tLCpk2bKCkpYevWrQAedUDSBIDOgp53gnquBV0z4LTVqwzR6T3X6PYpCcPVI8iZPn06FRUVlJaWUllZSWJiIsOHD8disXD33XezZs0aDAYDJSUlHD58mIyMvu8uvvjiCy699FKMRiPp6emceOKJfPvtt8yePZtrr70Wi8XCkiVLmDZtGiNHjmTfvn3cdtttLF68mNNOO20AzlrjNXHZYIoYOha6EOJJ4GygQkp5jLNWKBPnH8BZQAtwjZRyo68HqumC0/LNmBrYcfRGL5a0P7nooot47bXXKC8v55JLLgHg+eefp7Kykg0bNmA2m8nNze22bK4nLFy4kDVr1vDee+9xzTXXcMcdd3DVVVexefNmPvzwQx577DFeeeUVnnzySV+clsYfGAyQNEpNjDoJYUF3x4f+NHBGL++fCYxxPG4EHu3/sDR94oxw8XfIYghyySWX8NJLL/Haa69x0UWqSXZ9fT1paWmYzWZWrVrFwYMH3d7fCSecwMsvv4zNZqOyspI1a9YwZ84cDh48SHp6OjfccAPXX389GzdupKqqCrvdzoUXXsi9997Lxo3atgl6kkdBdacmJy5BD7049D4tdCnlGiFEbi+rnAc8K1XZxnVCiAQhRKaUspcC2pp+U74FotOCakIyWJg4cSKNjY1kZ2eTmakyWC+//HLOOeccJk+ezKxZszxqKHH++eezdu1apk6dihCCv/71r2RkZPDMM8/wwAMPYDabiYmJ4dlnn6WkpISlS5dit9sB+Mtf/uKXc9T4kJQxsGsF2CyqMXprHZij/Zt97Sd84UPPBoo6/V3seO0YQRdC3Iiy4snJ8XHd7qFGeYG2znthy5ajJ2NTUlJYu3Ztt+s2NTX1+roQggceeIAHHnjgqPevvvpqrr766mO201Z5iJE8GuxWqDukrPXWupC0zmGAwxallMullLOklLNSU1MH8tCDC2u76h06EBEuGs1gp2voYohmiYJvBL0E6Bz+MMzxmsZfVGxXFoW20DWa/qMF/SjeBq4SinlAvfaf+5mBSvnXaIYCUUlKwI8S9NB0ubgTtvgicBKQIoQoBn4LmAGklI8BK1Ahi3tQYYtL/TVYjYPyAgiLhcS8QI9EoxkcdC7SFcIWujtRLj2kAbrel8CtPhuRpm/KClTGpEFXbtBofELyaNi/RhXpCmFB14oQathtcHir9p9rNL4keZRqs9hSDbZ2LeiaAaJ6L1hatP/ch+gyuRrXxGjJBrXUgq4ZEMp1huhgw2q1BnoIGqegF69XSy3omgGhbLNq+ZbqfqbjUMKX5XOXLFnCzJkzmThxIsuXL3e9/sEHHzBjxgymTp3KKaecAqgkpKVLlzJ58mSmTJnC66+/Dhxt/b/22mtcc801AFxzzTXcfPPNzJ07l1/84hd88803zJ8/n+nTp3Pcccexa9cuAGw2Gz//+c+ZNGkSU6ZM4eGHH+bTTz9lyZIlrv1+/PHHnH/++d5/aBpIGqmWxd+qZYgKuq62GGqUF0DaBJWiHOTc/8397KzZ6dN9jk8azy/n/LLH931ZPvfJJ58kKSmJ1tZWZs+ezYUXXojdbueGG25gzZo15OXlUVNTA8Af//hH4uPjXRmqtbW1fZ5LcXExX331FUajkYaGBj7//HNMJhMrV67k7rvv5vXXX2f58uUcOHCATZs2YTKZqKmpITExkVtuuYXKykpSU1N56qmnuPbaaz35GDVdCYtWlRedLpcQ7CcKWtBDCylVhMv4xYEeSdDiy/K5y5Yt48033wSgqKiIwsJCKisrWbhwIXl5KmQ0KSkJgJUrV/LSSy+5tk1M7NvCu+iiizAajYAqHnb11VdTWFiIEAKLxeLa780334zJZDrqeFdeeSXPPfccS5cuZe3atTz77LOeflSariSPUpEuoC10zQDQUAKtNZAZxCVzO9GbJe1PfFE+d/Xq1axcuZK1a9cSFRXFSSed5FW53c53AF23j46Odj2/5557OPnkk3nzzTc5cOAAJ510Uq/7Xbp0Keeccw4RERFcdNFFLsHX9ANn6CKErKBrH3oooTNE3cIX5XPr6+tJTEwkKiqKnTt3sm7dOgDmzZvHmjVr2L9/P4DL5XLqqace5bt3ulzS09PZsWMHdrvdZe33dLzs7GwAnn76adfrp556Kv/+979dE6fO42VlZZGVlcW9997L0qU6l88nOCdGDWblgglBtKCHEuUFgID0iYEeSVDTU/nc9evXM3nyZJ599tk+y+eeccYZWK1WJkyYwF133cW8efMASE1NZfny5VxwwQVMnTrVdQfw61//mtraWiZNmsTUqVNZtWoVAPfddx9nn302xx13nGss3fGLX/yCX/3qV0yfPv2oqJfrr7+enJwcpkyZwtSpU3nhhRdc711++eUMHz6cCRMmePdBaY7GKeiRieBGa8JgRKhEz4Fn1qxZcv369QE5dsjy4mVQtRtuC97PbceOHVpgBogf/ehHTJ8+neuuu67b9/X/wkOq98LDMyBlHPzom0CPpkeEEBuklLO6e0873kKJ8i0wfHagR6EJAmbOnEl0dDR/+9vfAj2UwUNCDhhMIes/By3owY/NqiZDqwqh/hDM1uFpGtiwYUOghzD4MJoheQzEhG6vhpAT9B3VO3hrz1sYhHL/CyEQCAzCgBCCSGMkUeYoIk1qGW2KJsocRVZ0Ftmx2a7tgpbag/DlP6Bmr3peX6RqnzvJmR+4sbmJlLLX+G6N/wmUKzXkuehpMEcGehReE3KCXtpcyjv73gEJduxIKZGoL6/NbqPD3tHjtpGmSEYnjGZs4ljGJI5hTMIY8pPziQkLkloellZ48VKo2Qfp+ZA9AyaeD4m5kDhCTdrEDwv0KHslIiKC6upqkpOTtagHCCkl1dXVREREBHoooUdaaGdgD7pJUZvdRqu1lRZrCy2WFlqsLTRbmilqLGJ37W4KawvZXbubuvY6AMKN4SzKWcSS0UuYmzEXo8Ho8zG5zds/ho3PwOWvw5jvBW4c/cBisVBcXOxVzLbGd0RERDBs2DDM5uDPKNZ4xpCaFDUajMSExRxjdc/OODKZKKWkqrWK3bW7WV20mhX7V/D+/vfJiM7gnJHncN7o8xgRN2JgB17wqhLz438asmIOYDabXVmUGo1mYBl0Fro3tNvaWV20mrf2vMVXpV9hl3ZGJ4wmyhSF0WDEKIwYDUZMwkSYMYy8+DzGJo5lXOI4RsSPwGzopxVUtQeWnwjpk+Ca98A46K6zGo3GR/RmoWtB70JFSwXv7H2HjRUbsdltWKUVm92GTapHi6WFAw0HsDomKs0Gs8svf+6oc5mTOcezA1ra4PHvqUiWm7+A+Gw/nJVGoxksaEH3MRa7hf31+9ldu5vdNbvZVbuLbdXbqG+vZ3bGbG6ZeguzMrr9vI/l3Z/C+ifhsldh7Gn+HbhGowl5tKAPAO22dl7b/RqPb3mcqtYq5mbO5dZptzI9bbprnTZrGztrdrK5cjNbqrZQUbkdUV2IIW4YxqRRGIQBgzAQaYpkXNI4JiZPJD85n+TI5ACemWYo4gwo6I64sDjCjGF97qOurY5vyr9hf/1+Fo9czLDY4I7Q8oaSphJWHlzJsNhh5CflkxGd4ffoLi3oA0ibtY1Xdr3CE1ufoKathuOyjiMnNoeCqgJ21+zGKpWrJjMylZzaUmRYNPaMydiR2KUdu7TT0NHAwYaDrnDMjOgMJiZPZGLyRKakTmFyymSizFEDcj5VrVW8u/ddPi36FIEgLjyOuLA4YsNiXcvM6Ezy4vMYHjvcrR+6E+fk9P76/eyr30dVaxX5yfnMSJtBQojWow5Fmi3N7Kjewfbq7Wyr3sb26u0caDjQ4/omYSIvIY/xieMZn6Qe45LGEW4MZ2PFRtaVrePrsq/ZUb3D9R02GUxcOOZCbppyE6lRnifu1LfXs7VqKyVNJVjtVqx2Kxa7RT2XVozCSGZ0Jtkx2QyLHUZqZOpREWtWu5Xy5nKKGosobiqmpLGEFmsLFrsFi83i2pfFbmFa2jSunHAl5j56Dnxy6BPu+fIeGjsaXa/Fh8czPmk8E5ImMC5pHFnRWaRGpZIamUqEyTdhpFrQA0CLpYVXdr3CU9ueos3axqSUSUxOmcyU5IlMKdpMyhfLwGCAmz6HhOHHbN/U0cSOmqN/ZAcbVIVAozAyNnEs09KmMT1tOlNSp2Cz2yhtLqWsqYzS5lJKm0opby7Hare6RDguLM71PDkymZzYHEbEjSDafHRluQ5bB6uLVvO/vf/jy5IvsUkbE5MnEmWOorGjkYb2Bho7Gmm0NB61nUEYyI7JJi8+j9y4XBIjEl0/Pqvdik3asNqtNHQ0sL9+P/vr99Nkaer28xudMJpZ6bOYmTGTaanTjtrG+TjQcID48HgmJU9yfb6d8wosNguFdYVsr97uepQ0lWAQBkzChMlgUpPdBhPhxnBy43IZkzjGlaeQFZ3VrbVll3aaLc1Y7VZXQpsruQ1Bm62NypZKKloqqGqtoqKlgsrWSlosLQyLHUZuXC658bnkxuW6LsxSSsqby9les50d1TvYUbODfXX7ODPvTH40/UduJcQdajjE5srNzEifQXZMz3MxUkp21e7iowMfsapoFXvr9rqENz0qnfzkfPKT80mKSHKt79oWSUVLBTtrdrKrZhcVrRWu90zChFVaMRlMTE2dyrzMeczLnEdaVBpPbHmCNwrfwGQwcemES7l24rU9XrRtdht76/dSUFnA5srNbK7czP76/X2ef2dMBhOZ0ZmkRqZS0VJBWXMZNmk76v1oczRmgxmTwYTZYMZsMCOR7K/fz8j4kfx63q+Pio5zYrFZeGjjQ/x3+3+ZmDyRexfcS7O1mZ3VO9lRo/53hbWFWOyWo7aLNce6xH3xyMWcP8a7LlNa0AOIza6+REaDEfathvd/CZU7YdQpcOZfIWW02/uqb6+noLKATZWb2FSxiS1VW2i1th6znkCQGpVKZnQmZoOZho4G9Whv6PY2OjkimRFxI8iJy8EojHx88GMaOhpIi0rjvFHnce6oc8mNz+323Bo7GilpLlECW3/AJbQHGw7Sbmt3rWsymFwiGmWOIi8+j7y4PLV0PBIjEtlWtY0Nhzew/vB6vqv4rtvzy4rOIi8+jxFxI6htr2Vr1VaKGotc5z4yfiRhxjAK6wpdk9exYbHkJ+WTG5+LlBKrtB51sWmxtrC/fj8lTSWu48SYYxiZMBIDBposTTR2NNJsaabZ0uwSQHdJCE8gwhTB4ebDR22bFpVGRnQGhxoOuXIjDMLAyPiRJEYk8m35t5yeezp/Ov5PhBvDe9z/Rwc+4tdf/tr1eeXE5jAvcx7zs+YzO2M2cWFx7KjZwccHP+ajAx9xqPEQRmFkVsYsZqbPdLn3UiJTPDqvqtYqdtXsYmfNTho6GtRFOH1mt3eQRY1FPLrpUd7d9y7R5miuyL+ClIgUylvKKWsuo7y5nPLmcg63HHb93xLCE5iaOpUpqVOYmjqV3LhczMYjIuz8XlnsFsqayyhpKlGPRrWsbK0kLTKNYbHDGB47nGGxwxgWM4y0qLQec07WFK/hz1//mZKmEs4bdR4/m/UzEiNUfZeSphLu/OxOtlRt4fIJl3PHzDu6vSu12C0crD9IRUsFFa1HLuzO5Vl5Z3HZhMs8+qydaEEPNLUH4aP/gx3vqKzP0/8C487sd4lOq93K7trdbK3aSrgxnMzoTDJjMsmIyujxdtFit9DY0UhlSyWHGg9xsOEghxrUsqixiMaORk7OOZklo5YwN9P7RCu7tGOxWzAJk8uK9eb8dtbsZEvVFhLDE8mLzyMnLodI07Gp2XVtdWyr3saWqi1srdpKu62dCckTyE/OZ2LSRIbFDnNrDE0dTeyp2+NKQttXvw8hBLHmWJXfYI5xLU0Gk+tcnRnLdmknzBhGWlQaqZGppEWlkRKZ4vrRt9vaOdRwiAMNBzhQf4ADDQcoay4jJzaHCUkTGJ88nrGJY4k0RSKl5Jltz/C3DX9jetp0/nHyP1zC0vkzWvbdMp7a+hRTUqfw81k/Z3v1dtaWruXb8m9psbZgEAaSIpKoaq3CKIzMyZjDabmnsShnkcsKH0gKawt5ZNMjfHLoE0BZ9unR6aRHpZMRnUFGdAajE0YzNXUqw2OHByTjuNXayvKC5Ty99Wmiw6L52cyfERcexz1f3gMS/rDgD3xvRGDyRbSgB5Kv/gmf/hGEAU64A+bfBubgTcnWdViCjw8PfMjdn99NZkwm/zrlX+TE5QBQ21bLL9b8gnVl67h47MX8cs4vj7IWLXYLWyq3sLZsLQfqDzA/az4nDz/5mItCoChtKsVkMJEckRzYDO1e2FO7hz+u+yMbKzYCkJ+cz4MnPsjw2GPdpAOFFvRAsekFeOuHMO4sOOuBoK/DogleNlVs4sef/hiJ5OFFD2M2mvnpqp9S3VrNr+f92mt/rKZv7NLOO3vfobSplOsmX+fRxL8/0IIeCIrXw1NnwfA5cOWbqjSnRtMPDjUc4pZPbqGsqQyApMgkHjrpISalTArwyDQDSW+CHuS1ZEOUhjJ46XKIzYCLn9VirvEJOXE5PHfmc8xMn8mczDm8fPbLWsw1R6GLhvgaSxu8fDm0N8KVb0DUwE86aQYvCREJLD9teaCHoQlStKD7EinhnduhZANc8pxu5qzRaAYU7XLxJWsfgYKX4KS7YcI5gR6NRqMZYmhB9xV7VsLH98CEc2HhnYEejUajGYJol4s32Cwq27N0E5RtgrLN6pGWD0seVSn9Go1GM8BoQfeEg2vh498o8XamtYfFQuYUmHMjzL8VwoOkP6lGoxlyaEF3Bynhq4dh5e9UctCcGyBrOmROg6SR2iLXaDRBQUgK+t7KJkalDpAl3FoHb90Cu95T/vHz/gkR8QNzbI1Go/GAkDMtX99QzGkPreGLwir/H6x0E/x7IRR+qApqXfysFnONRhO0hJygnz4pg9GpMdzy/Ab2VzX75yBSwvqn4InTwG6Fa1bA/Fv6XR1Ro9Fo/Ilbgi6EOEMIsUsIsUcIcVc3718jhKgUQmxyPK73/VAVMeEmHr96Fiajgeue+Zb6VkvfG7mDlHB4G6y+Dx5dAO/+BHIXwE1rIGeub46h0Wg0fqRPH7oQwgg8ApwKFAPfCiHellJu77Lqy1LKH/lhjMcwPCmKRy+fweWPf81tL37Hkw6B9xi7HUq/gx3/U7XKa/YBAnLmweK/wcylEKRlPTUajaYr7kyKzgH2SCn3AQghXgLOA7oK+oAyd2Qy9y6ZxF1vbOHPK3bym3Py+95ISqjeA/vXqMeBL6ClCgwmyFsIx90G4xZDbLr/T0Cj0Wh8jDuCng0Udfq7GOjOB3GhEGIhsBv4qZSyqJt1+k9DGRStg8yp/GB2HrsON/Lkl/sZlxHDJbNzjl7XZoXKHaqU7cEvYf/n0FSu3ovNgtGnwKhFMPZ0iAyOov8ajUbjLb4KW3wHeFFK2S6EuAl4BljUdSUhxI3AjQA5OTld33aP/Z/Bmzep5+Hx3JMxmdkp6Xzyvy+ZaDiLSZHVSsBLNih3isXRQzM6DfJOgNwTlDWeNFJPcmo0mkFFnw0uhBDzgd9JKU93/P0rACnlX3pY3wjUSCl7je/zusGFtR0qth9Jty/bjCzfiujUkNhuCENkTEYMmwXDZkH2TC3gGo1mUNBbgwt3LPRvgTFCiDygBPgBcFS7aiFEppSyzPHnucCOfoy3d0zhKksza/qR49ssFBVu4j+vr6CgKYHtcgThpZFMNyUyMyyRmZHxTI6yEh+pG01oNJrBi1st6IQQZwF/B4zAk1LKPwkh/gCsl1K+LYT4C0rIrUAN8EMp5c7e9umPFnRSSg5Ut7DxYC0bDtWy8WAtuw434jzFEclRTMqKZ2J2nFpmxZEcE+7TMWg0Go0/GdI9RRvaLGw6VMeWknq2ldaztaSBQzUtrveHJ0Vy/OgUFoxO4bhRKSRFB7YBrEaj0fTGkBb07qhvsbCtrJ5tJQ18c6CGdXuraWy3IgTkZ8Zx/OgUThyXyry8ZAwG7XfXaDTBgxb0PrDa7BSU1PPVniq+2FPFhoO1WGySkSnRXDl/BBfOHEZchPa/azSawKMF3UNaOqx8tO0wz6w9wHeH6ogKM3LBjGyump/L2PTYQA9Po9EMYbSg94OC4jqe+eog7xSU0mG1c9yoZH540iiOH52C0GGQGo1mgNGC7gOqm9p5eX0Rz351kPKGNmbkJPDjU8Zw4thULewajWbA0ILuQ9qtNl5dX8yjq/dSUtfK1OEJ3H7KaE4el6aFXaPR+B0t6H6gw2rn9Y3FPLJqD8W1rUzKjuP3505i5ghdE0aj0fiP3gQ95BpcBAthJgOXzslh1c9P4q/fn0Jts4WL/72WR1btwWYPzEVSo9EMbbSg9xOz0cDFs4bz/k9O4MxJGTzw4S6uevJrKhraAj00jUYzxNCC7iPiIsw8fOl07r9wMhsO1nLmPz5n1a6KQA9Lo9EMIbSg+xAhBJfMzuHd244nNTacpU99y73vbqfDag/00DQazRBAC7ofGJ0Wy1u3LuCq+SN4/Iv9nPPwF3x7oCbQw9JoNIMcLeh+IsJs5A/nTeKJq2fR1G7losfWcuerm6luau97Y41Go/ECLeh+5pQJ6Xx8x0J+eNIo3vyuhEV/+4wXvj6EXUfCaDQaH6MFfQCICjPxyzPG8/7tJzAhM5a739zCBY9+xdaS+kAPTaPRDCK0oA8gY9JjefGGeTx0yVSKa1s4++Ev+PGL33GwujnQQ9NoNIMAXzWJ1riJEILzpw/jlAnp/PuzvTzxxX5WbCnjsrk5/GjRaNJiIwI9RI1GE6Lo1P8AU9HQxrJPC3npmyLMRgPXn5DHDQtH6vrrGo2mW3QtlxDgQFUzf/t4N+9sLiUxyswdp43jsjk5GHXHJI1G0wldyyUEyE2J5uFLp/PubcczLiOWe97ayuJln7NuX3Wgh6bRaEIELehBxqTseF68YR7/unwGjW1WfrB8Hbc+v5Hi2pYet+mw2gnUnZZGowke9KRoECKE4KzJmSwan8a/P9vHo5/tYeWOwyxdkEdshInSulbK6ttcy/pWC7efMoafnjo20EPXaDQBRPvQQ4CSulb+smIH7xaUAZAQZSYzPpKs+AgyEyI4UNXC2n3VvPHD45g6PCHAo9VoNP5ET4oOEqqa2okOMxEZZjzq9fpWC6c/tIbYCBPv3HY8EWZjD3vQaDShjp4UHSSkxIQfI+YA8ZFm7rtwMoUVTfx9ZWEARqbRaIIBLeiDhJPGpXHJrOEsX7OX7w7VBno4Go0mAGhBH0T839kTyIiL4OevbqbNYgv0cDQazQCjBX0QERdh5v7vT2FvZTP/7+PdgR6ORqMZYLSgDzJOGJPKpXNy+M/n+9hwUDfV0GiGElrQByH/t3gCWfGR/PzVAlo7tOtFoxkqaEEfhMSEm/jr96ewv6qZO1/bTFO7NdBD0mg0A4AW9EHKgtEp/Py0sby3pYzTH1rD54WVgR6SRqPxM1rQBzE/WjSG124+jnCzeDc7FAAADNJJREFUgSuf+Ia7Xi+goc3i0T4a2ix8sLWc+z/YyepdFbp1nkYTxOhM0SFAm8XGQyt38581+0iPi+DPF0zm5HFp3a5rtdnZXFzP54WVfF5YxaaiOmydRHx4UiSXzsnhopnDSY0NH6hT0Gg0DnTqvwaATUV13PnqZgormpiTl4TZKGi32Gm32mm32mi32qlu6qCp3YoQMGVYAgvHpHDCmFQmZcfxyY4Knv/6IOv21WA2Ck6bmMHlc3KYPyoZIXTddo1mINCCrnHRbrXxyKq9fLargjCTgXCTkXCTwfHcQFykmbl5ySwYnUxCVFi3+9hT0cSL3xzitQ3F1LdamDUikT9fMJmx6bEDfDYazdBDC7rGL7RZbLyxsYQHPtxJY5uVm04cyW2LxujiYBqNH9HFuTR+IcJs5LK5OXzys5NYMj2bR1bt5TQdUaPRBAwt6Jp+kxQdxoMXTeWFG+ZiMgiufOIbbn/pu167LGk0Gt/jlstFCHEG8A/ACDwupbyvy/vhwLPATKAauERKeaC3fWqXy+CkzWLj0dV7eXT1XjpsdtJiw5kyLIFpw+OZMiyBKcPie/TN+wspJU3tVioa26lvtWAUAqNBYDIKTAaB0WDAbBQkRoURFWbUE7wan9LSYaW8vo3DDe0cbmijvKGNKdnxHDc6xav99eZy6bMFnRDCCDwCnAoUA98KId6WUm7vtNp1QK2UcrQQ4gfA/cAlXo1WE9JEmI389NSxXDAjm093VlBQXM/m4jpW7jjsWic7IZKcpCiGJzmXUQxLjGJYYiTR4SYiTAZMRvduHtssNvUjqVc/FOfycEMblY3tVDS2U9HQTqub1ScjzAaSo8NJjgkjOTqMxOgwDEJgsdmx2Ox0WCVWu3ouEJiNApPRQJhRXRTMRgPSMS71sNNqsdHaYcNml8RGmIiPNBMfaSbO8UiINJMSG05qTDhpceGkxoYTG27q9sIipaTdqo4PIAEp1ROJxGaXWGxSjdVmp8OxrtUuMRsMmE1qjGFGNRFuNAia2qzUt1qoa7VQ19JBQ6uFuhYLzR0213k4z6HVYsNkECREhREfaSYhSo0/ISqMyDCj4/h2rDbn56TGW9fSQXVzBzVNHdS0dFDT3EFdi8XRfSuC7IRIshIiyYyPICshEovNftT/tKxeLe1Skp0YSXZCpOs7k50YSUZcBJFmI+FmNdFvNHR/UXaOr8Nmx2aT2KXELnEs1edndYzZGfmlIsFsWHvIwbDaJDUtHVQ3tVPdpM6tqqmd6uYODje00dh2bKb2TQtHei3ovdGnhS6EmA/8Tkp5uuPvXwFIKf/SaZ0PHeusFUKYgHIgVfayc22hDy3qWy1sLVHivqu8kaKaFg7VtFLV1N7t+mFGAxFmA5FhRiLMRqRUMfJWu1QPx/OWbmrVRIcZSY+PIC02nLRYxzJOPY+PMiOldAiOetjsSvhqWyzqR9nc4fph1jR3ALjEWom3cF1wLA7RtDqEwmK1I0GN22QkMsxIpNlIhNmAQQgaHeLZ0GahvtXS7Y8d1IUlJSYcgxAuUVUiY/fNP8QNwkwGIkyGTuegHja7VBeAlg4aehh/VwwCEqPCSHJcJJOjw0iIMlPbbKGsvpWSurZuvwtCqMYuGXERZMRHYBSCkrpWimtbqG3pOUnOZBCEmwyYTQZsNkmH44Ls77y4hCgzydFhJMeEkxwdRnpchOOhziHNcR4x4d63c+6XhQ5kA0Wd/i4G5va0jpTSKoSoB5KBqi4DuRG4ESAnJ8etwWsGB/GRZhaMTmFBF6uktcNGcW0LRbUtlNa1uaxAp0XotA4NDjeJ2ehwlxiUdZkYZSbd8SPJjFc/ntgIc4DO0nNsdklDq4WqJsfdRKPjzqKh3SVwTiENNxkId1wczAZ1QXEa8UIIBGA0CMJMBsyOOwanJW4wCGzHWO7Kio4OMylLO8pMfGSY6w4izNT3XZJz/HWtFprbrQ43lsF152J2jCc2wtyj1ezEebdVUtdKmNFARnwEabERPY6jqd1KSW0rJXUtVDS0H3XRa7faaLeoc3V+Juou6shnYzSohxACgwCjEBiEwGwSXcJ51XO17rHjMBqEulBFhWF2887SX3h/mfACKeVyYDkoC30gj60JTiLDjIxJj2XMEI1hNxoEiQ6rNRQ/g87j7y8RZiMjkqMZkRzt1vox4SbGZcQyLiP0Pjd/4c7lpAQY3unvYY7Xul3H4XKJR02OajQajWaAcEfQvwXGCCHyhBBhwA+At7us8zZwteP594FPe/OfazQajcb39OlycfjEfwR8iApbfFJKuU0I8QdgvZTybeAJ4L9CiD1ADUr0NRqNRjOAuOVDl1KuAFZ0ee03nZ63ARf5dmgajUaj8QSdKarRaDSDBC3oGo1GM0jQgq7RaDSDBC3oGo1GM0gIWD10IUQlcNDLzVPokoU6hBiq567Pe2ihz7tnRkgpU7t7I2CC3h+EEOt7qmUw2Bmq567Pe2ihz9s7tMtFo9FoBgla0DUajWaQEKqCvjzQAwggQ/Xc9XkPLfR5e0FI+tA1Go1GcyyhaqFrNBqNpgta0DUajWaQEHKCLoQ4QwixSwixRwhxV6DH4y+EEE8KISqEEFs7vZYkhPhYCFHoWCYGcoz+QAgxXAixSgixXQixTQhxu+P1QX3uQogIIcQ3QojNjvP+veP1PCHE147v+8uOEtaDDiGEUQjxnRDiXcffg/68hRAHhBBbhBCbhBDrHa/163seUoLeqWH1mUA+cKkQIj+wo/IbTwNndHntLuATKeUY4BPH34MNK/AzKWU+MA+41fE/Huzn3g4sklJOBaYBZwgh5qEarj8kpRwN1KIasg9Gbgd2dPp7qJz3yVLKaZ1iz/v1PQ8pQQfmAHuklPuklB3AS8B5AR6TX5BSrkHVlu/MecAzjufPAEsGdFADgJSyTEq50fG8EfUjz2aQn7tUNDn+NDseElgEvOZ4fdCdN4AQYhiwGHjc8bdgCJx3D/Trex5qgt5dw+rsAI0lEKRLKcscz8uB9EAOxt8IIXKB6cDXDIFzd7gdNgEVwMfAXqBOSml1rDJYv+9/B34B2B1/JzM0zlsCHwkhNgghbnS81q/v+YA2idb4DimlFEIM2phTIUQM8DrwEyllg+jUbn2wnruU0gZME0IkAG8C4wM8JL8jhDgbqJBSbhBCnBTo8Qwwx0spS4QQacDHQoidnd/05nseaha6Ow2rBzOHhRCZAI5lRYDH4xeEEGaUmD8vpXzD8fKQOHcAKWUdsAqYDyT8//buJtSqKgzj+P/BosQKMQ28ftCXgyZ3Vg0yiEoHolFRgRh0oZo2EAtyIhRmIyEyCKpZmNxBHwoRGRb0AQXmoCALBEOMEjUjMSLiabDWoZ1plIdzTmed5webe/bZ9x7WgnXfu3j33e9bG69Dm+v9FuAuSYcpKdTbgedof97YPlq/HqP8Ab+JPtf5uAX0f9OwumXdZtwPAW+NcCwDUfOnrwBf2d7eudT03CUtqjtzJM0FVlHuH7xPabwODc7b9pO2l9q+mvL7vM/2Bhqft6R5ki7vvQZWA1/S5zofuydFJa2h5Nx6Dau3jnhIAyHpNeA2SjnNH4AtwJvALLCcUnr4Adtn3zgda5JWAh8CX/BnTnUzJY/e7NwlTVNugs2hbLRmbT8l6VrKznUBcAB40Pavoxvp4NSUyybba1ufd53fG/X0ImCn7a2SrqSPdT52AT0iIs5t3FIuERFxHgnoERGNSECPiGhEAnpERCMS0CMiGpGAHtEHSZZ0/ajHEQEJ6NGYWpL0F0mnO8eOUY8rYhhSyyVatM72e6MeRMSwZYceE0HSjKSPJe2Q9JOkg5Lu6FyfkrRb0snaVOHRzrU5kjZLOiTp51odr1tT6M7akOCUpBfUrSQWMUTZocckuZlSY3shcC/wuqRr6qPVuyi1NKYoVQ73Sjpkex+wEVgPrAG+AaaBM53PXQvcCFwB7Af2AO8MZUYRHXn0P5pSq/YtpHQ+6nkc+A14BljiuuglfQY8D3wAHAbm16YaSNoGLLY9I+lr4AnbfyuUVMub3mr7o3o+C3xu+9mBTDDiHyTlEi262/b8zvFSff+o/7qD+ZayI58CTvaCeedar6nCMkqzifP5vvP6DHBZf8OPuDAJ6DFJlpyV314OfFePBb1ypp1rvRrcR4DrhjPEiAuXgB6T5CrgMUkXS7ofuAF42/YR4BNgm6RLaynbh4FX68+9DDwtaYWK6VrmNOJ/JTdFo0V7JP3eOd9LaRTwKbACOE6pMX+f7RP1e9YDL1J26z8CWzr/+rgduAR4l5KfPwjcM+hJRPxXuSkaE0HSDPCI7ZWjHkvEoCTlEhHRiAT0iIhGJOUSEdGI7NAjIhqRgB4R0YgE9IiIRiSgR0Q0IgE9IqIRfwBHq3dFY8pfEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(trn_loss, val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still getting low training loss, but high validation loss and poor validation accuracy.\n",
    "\n",
    "Let's try adding more data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100x as much data as before\n",
    "TRN_EXAMPLES = 12800\n",
    "VAL_EXAMPLES = 1200\n",
    "# Other parameters are kept the same\n",
    "SEQ_LENGTH = 10\n",
    "VOCAB_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "\n",
    "train_ds = ToyDataset(num_examples=TRN_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "valid_ds = ToyDataset(num_examples=VAL_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel(VOCAB_SIZE)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We use RAdam to avoid having to use warmup\n",
    "# If we use regular Adam, this usually won't converge for long sequences\n",
    "optimizer = RAdam(model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.844509482383728\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.4921850264072418\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.4671918749809265\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.4769051969051361\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.7999999\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.47771650552749634\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.481773316860199\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.45999717712402344\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.46936410665512085\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.7999999\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.48127609491348267\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.4805912375450134\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.45702213048934937\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.4726109504699707\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.7999999\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.4735395908355713\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.4819353520870209\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.442431777715683\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.47053855657577515\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.7999999\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.4508120119571686\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.23643401265144348\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.13270068168640137\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.16700567305088043\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.9610197\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.13277581334114075\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.123570516705513\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.0518583245575428\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.06496930867433548\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.9705593\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.03152110055088997\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.07032491266727448\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.04852878302335739\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.07157288491725922\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.9640625\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.042034950107336044\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.0383387915790081\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.03337687999010086\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.06476191431283951\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.974671\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.028005003929138184\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.011802737601101398\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.03377469256520271\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.012279827147722244\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.9692435\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.08494352549314499\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.0128243463113904\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.01555594988167286\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.009458519518375397\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.9736842\n"
     ]
    }
   ],
   "source": [
    "trn_loss, val_loss, val_acc = train(model, train_dl, valid_dl, loss_fn, optimizer, num_epochs=10, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(trn_loss, val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like we need much more data in order to get our network to learn the solution. It also never quite reaches 100% accuracy. Perhaps lowering the learning rate would have helped at the end? \n",
    "\n",
    "Most of the learning seems to have happened between epochs 3 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we increase the sequence length? Does the problem become harder? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10x sequence length \n",
    "SEQ_LENGTH = 100\n",
    "# Other parameters are kept the same (as the working version)\n",
    "TRN_EXAMPLES = 12800\n",
    "VAL_EXAMPLES = 1200\n",
    "VOCAB_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "\n",
    "train_ds = ToyDataset(num_examples=TRN_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "valid_ds = ToyDataset(num_examples=VAL_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel(VOCAB_SIZE)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We use RAdam to avoid having to use warmup\n",
    "# If we use regular Adam, this usually won't converge for long sequences\n",
    "optimizer = RAdam(model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.8871000409126282\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1d5c8397753e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrn_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/bio-experiments/utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dl, valid_dl, loss_fn, optimizer, num_epochs, print_every)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\tStep:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\tLoss:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trn_loss, val_loss, val_acc = train(model, train_dl, valid_dl, loss_fn, optimizer, num_epochs=4, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(trn_loss, val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this looks surprisingly good. Despite having longer sequences we're achieving comparable or even better validation accuracy as in our previous example. But the truth is that we're being misled, let's take a look at an example from our **training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\t tensor([497, 375, 957, 212, 170, 535, 740, 292, 758, 115, 808, 945,  37, 298,\n",
      "        268, 673, 474, 467,  48, 536, 715, 892, 650, 925, 198, 105, 131, 288,\n",
      "        206, 144,  73, 213, 397, 170, 495,  62, 880, 122, 392, 285, 975, 183,\n",
      "        224, 321, 331, 485, 107, 560, 825, 204, 520, 695, 859, 595, 821, 338,\n",
      "        869, 494, 747, 960,  54, 666,  65, 841, 824, 460, 737, 395,  37, 342,\n",
      "        738, 738, 690, 747, 461, 952,  16,  72, 262, 976,  53, 309, 909, 740,\n",
      "         17, 660, 845, 683,  96, 229,  69,  37, 985, 726, 646, 648, 334, 215,\n",
      "        433, 220], device='cuda:0')\n",
      "y:\t tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0')\n",
      "y_hat:\t tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Check if we're learned how to perform on items in the train dataset\n",
    "x, y = train_ds[0]\n",
    "y_hat = get_output_for_example(model, x)\n",
    "\n",
    "print(\"X:\\t\", x)\n",
    "print(\"y:\\t\", y)\n",
    "print(\"y_hat:\\t\", y_hat.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has learned to output `0` for all positions which gives it a very low loss as well as a high accuracy.\n",
    "\n",
    "Our validation accuracy is ~98% which makes sense because 98% of all outputs are `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: How do we fix this? Focal Loss?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using [Focal Loss](https://arxiv.org/pdf/1708.02002.pdf).\n",
    "\n",
    "> Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10x sequence length \n",
    "SEQ_LENGTH = 100\n",
    "# Other parameters are kept the same (as the working version)\n",
    "TRN_EXAMPLES = 12800\n",
    "VAL_EXAMPLES = 1200\n",
    "VOCAB_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "\n",
    "train_ds = ToyDataset(num_examples=TRN_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "valid_ds = ToyDataset(num_examples=VAL_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.25 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.06355327367782593\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.031293246895074844\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.030473466962575912\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.03020532988011837\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.03026038594543934\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.030119257047772408\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.03018295206129551\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.030604461207985878\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.030097953975200653\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.030159911140799522\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.030336039140820503\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.03026892989873886\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.029794353991746902\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.02998814918100834\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.03024820238351822\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.030136512592434883\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.028880801051855087\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.029975861310958862\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.02945110946893692\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.03019261732697487\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.02857033908367157\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.029831036925315857\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.029460813850164413\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.030065931379795074\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.028599146753549576\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.029335277155041695\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.028905410319566727\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.029810570180416107\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.029651707038283348\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.029846271499991417\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.029009031131863594\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.029809312894940376\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.029850197955965996\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.02934572845697403\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.028549203649163246\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.02978997677564621\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.030158232897520065\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.02883555367588997\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.02939988113939762\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.029712030664086342\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.25 2\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.07901634275913239\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.017104078084230423\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.016643047332763672\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.01639988087117672\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.016451269388198853\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.01646561175584793\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.016099713742733\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.01622726395726204\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.016228020191192627\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.01615048199892044\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.016193198040127754\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.01631029322743416\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.015978267416357994\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.016041111201047897\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.016185300424695015\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.016387464478611946\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.015853099524974823\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.01616222783923149\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.016203129664063454\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.016307996585965157\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.015740273520350456\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.016066808253526688\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.01599160209298134\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.016143742948770523\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.015555414371192455\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.01587548851966858\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.015898725017905235\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.016115115955471992\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.015348532237112522\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.015788862481713295\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.015881480649113655\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.01626313477754593\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.01552734337747097\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.01566968485713005\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.015804225578904152\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.01637507602572441\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.015731586143374443\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.01566203497350216\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.015500632114708424\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.016132785007357597\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.25 5\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.008258577436208725\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.002664546249434352\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.0024958725553005934\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.0024785581044852734\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.002293984405696392\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.0022877135779708624\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.0023757091257721186\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.002299079205840826\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.002265940885990858\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.002258792519569397\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.002264426089823246\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.0023253050167113543\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.0022849412634968758\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.002283879090100527\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.002264844486489892\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.002265036106109619\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.0022412908729165792\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.002261694287881255\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.002257927553728223\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.00228109466843307\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.0022489596158266068\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.0022977516055107117\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.002279008040204644\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.0022542611695826054\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.0022015238646417856\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.002248273929581046\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.002256401814520359\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.0022826131898909807\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.0022104759700596333\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.0022503589279949665\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.0022625501733273268\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.00224853353574872\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.002199087757617235\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.002251890255138278\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.0022510187700390816\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.002258904045447707\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.0021930693183094263\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.0022435642313212156\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.00225223321467638\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.002268682001158595\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.25 10\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.001553740119561553\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.00013904183288104832\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.00010801962343975902\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.00010045401722891256\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.96494246\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 9.937271533999592e-05\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 8.73159006005153e-05\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 8.448566950391978e-05\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 8.526694728061557e-05\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.9795311\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 8.179125870810822e-05\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 8.210584928747267e-05\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 7.944901881273836e-05\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 7.914377783890814e-05\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 7.979981455719098e-05\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 7.955604087328538e-05\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 7.64866781537421e-05\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 7.877016469137743e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 7.789798110025004e-05\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 7.566453132312745e-05\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 7.72170678828843e-05\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 7.567787542939186e-05\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 7.560307858511806e-05\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 7.726548938080668e-05\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 7.435993029503152e-05\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 7.744642061879858e-05\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 7.646730227861553e-05\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 7.617137453053147e-05\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 7.742660091025755e-05\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 7.766042836010456e-05\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 7.64544092817232e-05\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 7.500730134779587e-05\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 7.559012738056481e-05\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 7.718690176261589e-05\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 7.514249591622502e-05\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 7.589605229441077e-05\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 7.558667130069807e-05\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 7.638839451828972e-05\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 7.455293234670535e-05\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 7.636083319084719e-05\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 7.667377212783322e-05\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 7.666455348953605e-05\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.25 20\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.0006800943519920111\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 1.2398800208757166e-06\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 1.0450389709149022e-06\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 7.30350109279243e-07\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.7411842\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 9.37150787194696e-07\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 7.283236413968552e-07\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 5.780476612926577e-07\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 4.2574058056743525e-07\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.763824\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 5.066303288003837e-07\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 9.340448059447226e-07\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 4.1827144059425336e-07\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 4.6260166186584684e-07\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.76548517\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 3.635392147316452e-07\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 3.4414608762745047e-07\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 4.5189858610683586e-07\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 2.7644830424833344e-07\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.7949259\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 4.820176400244236e-07\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 3.516096853672934e-07\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 3.152352121560398e-07\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 3.197644389274501e-07\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.8095805\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 2.36359113614526e-07\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 3.291572170383006e-07\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 2.505712757283618e-07\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 2.762807298495318e-07\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.8094572\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 2.052252767725804e-07\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 2.465432942244661e-07\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 2.196295270096016e-07\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 2.1176684583679162e-07\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.821949\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 1.9671482220928738e-07\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 2.0781422449545062e-07\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 1.9460804878690396e-07\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 2.2817771139216347e-07\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.8632648\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 2.1920381243489828e-07\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 1.4920387059191853e-07\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 1.5733458269551193e-07\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 1.662621968989697e-07\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.85851157\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 1.739024071412132e-07\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 1.7492580184352846e-07\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 1.7781468386601773e-07\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 1.6184633011562255e-07\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.87652135\n",
      "\n",
      "0.25 50\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 2.1692963855457492e-05\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 7.721297379248426e-09\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 3.6339398157281266e-09\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 1.282970396587757e-09\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.5218257\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 1.3423814282376156e-09\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 1.1613242589803008e-09\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 8.605942625194984e-10\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 1.8412499303011032e-09\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.6419161\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 1.469784738361568e-09\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 1.745686095677712e-10\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 8.014498786845792e-11\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 1.7788595041423605e-09\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.5928783\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 3.8661389645255895e-09\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 2.1873652888970696e-10\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 5.529237831480849e-11\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 7.968128379332029e-09\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.6219654\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 3.415462357381216e-10\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 5.280924031403345e-10\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 3.429959441469954e-11\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 2.0154075630607338e-10\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.6625987\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 1.012021230706317e-10\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 5.05294972530379e-10\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 4.754351135005663e-09\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 2.39206432439687e-10\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.61258215\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 1.3224404904477183e-10\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 1.53827353632785e-10\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 7.707638693954522e-11\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 4.3868414456582627e-10\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.5938404\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 5.931755087118518e-10\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 1.3640309715068355e-10\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 2.3243998392707965e-10\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 1.2023747275602403e-10\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.6622862\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 3.153653782561605e-09\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 3.397226944201748e-10\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 2.499704332414865e-10\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 1.5214995929824227e-11\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.64104444\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 3.795862513200632e-10\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 1.6178324305227143e-11\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 4.66644813068573e-11\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 3.423267572189026e-11\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.6290872\n",
      "\n",
      "0.5 1\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.0946936383843422\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.02620142139494419\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.02658798359334469\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.026166332885622978\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.026492711156606674\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.026514504104852676\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.026175832375884056\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.026577908545732498\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.02626199647784233\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.026385044679045677\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.02618236094713211\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.026513058692216873\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.026283569633960724\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.02637074887752533\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.02605086751282215\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.026372548192739487\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.025798043236136436\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.026173891499638557\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.02600572444498539\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.02625136636197567\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.025643600150942802\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.025853710249066353\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.0255973432213068\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.026336468756198883\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.025265753269195557\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.02568730339407921\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.025813354179263115\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.026234960183501244\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.025567902252078056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.025706419721245766\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.02599986083805561\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.026483148336410522\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.02530304528772831\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.025322960689663887\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.026116466149687767\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.026376737281680107\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.025840088725090027\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.024533485993742943\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.0255404282361269\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.026015667244791985\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.5 2\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.10018294304609299\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.01541371550410986\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.015286731533706188\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.01506084855645895\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.014915461651980877\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.015036881901323795\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.01504986360669136\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.015076221898198128\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.014968044124543667\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.014876498840749264\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.015069116838276386\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.015113298781216145\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.01483715046197176\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.014931373298168182\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.014994057826697826\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.015035445801913738\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.0147164985537529\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.014821612276136875\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.015008378773927689\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.01496872864663601\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.014572450891137123\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.014767805114388466\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.014888965524733067\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.014962558634579182\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.01459056418389082\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.014632366597652435\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.014704780653119087\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.014872642233967781\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.014528193511068821\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.01479664258658886\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.014628936536610126\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.014854680746793747\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.01436414010822773\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.014671879820525646\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.014519378542900085\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.014947700314223766\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.014515537768602371\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.014704504050314426\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.014428908936679363\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.014904133975505829\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.5 5\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.05523524060845375\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.002676208969205618\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.002661551581695676\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.0024422674905508757\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.0024396234657615423\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.0022404168266803026\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.0024071510415524244\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.0023551699705421925\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.002333223819732666\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.002347129862755537\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.002338240621611476\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.002348462352529168\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.002288221614435315\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.002300478518009186\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.002271454082801938\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.002332402626052499\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.002304996829479933\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.002299634274095297\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.0023001073859632015\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.0023029251024127007\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.0022431330289691687\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.002300053369253874\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.0022716789972037077\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.0022979730274528265\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.002244953066110611\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.0022849987726658583\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.0022670025937259197\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.002283199690282345\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.002236428204923868\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.0022816259879618883\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.0022839768789708614\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.0023135291412472725\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.002198261208832264\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.0022927955724298954\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.0022662822157144547\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.002332764444872737\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.0022162040695548058\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.002272562123835087\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.0022514325100928545\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.002308336552232504\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.5 10\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.009036353789269924\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.00015857229300308973\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.00014035565254744142\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.00013125657278578728\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.97126627\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.00012311022146604955\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.00010793653200380504\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.00010133771866094321\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 9.674580360297114e-05\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97930086\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 9.550907270750031e-05\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 9.137110464507714e-05\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 9.500901796855032e-05\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 9.590420813765377e-05\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.9799259\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 9.083004260901362e-05\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 8.769368287175894e-05\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 8.853153849486262e-05\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 8.66769696585834e-05\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 8.486948354402557e-05\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 8.706250082468614e-05\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 8.290402183774859e-05\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 8.451673784293234e-05\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 8.236408029915765e-05\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 8.364292443729937e-05\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 8.762967627262697e-05\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 8.206839265767485e-05\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 8.2134545664303e-05\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 8.28961274237372e-05\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 8.152214286383241e-05\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 8.479263487970456e-05\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 8.078146493062377e-05\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 8.235565474024042e-05\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 8.26483010314405e-05\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 8.168055501300842e-05\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 8.071278716670349e-05\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 8.181852899724618e-05\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 8.245380740845576e-05\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 8.140734280459583e-05\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 7.947905396576971e-05\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 8.056686783675104e-05\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 8.246384095400572e-05\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 8.162444282788783e-05\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.5 20\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.007844188250601292\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 2.5292915779573377e-06\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 1.602833322067454e-06\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 1.1974468634434743e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.81050986\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 8.357553156201902e-07\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 1.185572500617127e-06\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 1.2188603477625293e-06\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 1.0914271797446418e-06\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.8278536\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 6.795280569349416e-07\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 1.0412363735667896e-06\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 1.1116173936898122e-06\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 8.544099614482548e-07\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.8376562\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 5.399464839683787e-07\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 5.091142156743445e-07\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 6.456002097365854e-07\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 6.621511374760303e-07\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.8451481\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 6.601227369174012e-07\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 5.302244971971959e-07\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 5.579763069363253e-07\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 5.912812071073859e-07\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.85717934\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 5.048344178248954e-07\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 4.6707813794455433e-07\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 4.5146006755203416e-07\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 4.690851369559823e-07\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.84671885\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 6.697403023281367e-07\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 4.501653165789321e-07\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 3.380630744231894e-07\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 3.642902299816342e-07\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.87081397\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 3.5705392065210617e-07\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 4.3224781620665453e-07\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 3.698027910559176e-07\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 3.6091068977839313e-07\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.8645888\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 2.716546134706732e-07\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 3.176485847689037e-07\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 3.9828404396757833e-07\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 3.911541170964483e-07\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.8823766\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 2.2442262093136378e-07\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 2.5824977001320804e-07\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 2.833652104072826e-07\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 2.689816085421626e-07\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.88311666\n",
      "\n",
      "0.5 50\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 3.127283605408593e-07\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 8.443802990143467e-09\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 2.2186062764717462e-08\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 1.1415545175808006e-09\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.7004111\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 1.4972692530257348e-10\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 1.4948040583107058e-09\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 1.0307932285513743e-08\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 6.185766343591581e-10\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.6546299\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 1.2428077456050346e-09\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 1.6657922263796365e-10\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 8.689247654736221e-10\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 1.81540463217722e-10\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.6943668\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 3.995478947693698e-11\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 3.039579976604756e-10\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 6.626790782782166e-11\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 1.1444150072037473e-09\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.6713322\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 2.2894248996596644e-10\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 3.3048541681068855e-10\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 1.1142899936089634e-09\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 6.456367107610106e-10\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.70546865\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 1.6230171720477138e-11\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 2.4916186891488223e-09\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 8.115895455684807e-11\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 7.538671770168648e-11\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.6791035\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 2.5028814090721774e-11\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 1.5162476829644334e-10\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 1.471602284475182e-10\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 4.4453878078609677e-11\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.6915707\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 5.940740122056809e-10\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 3.244697399651386e-09\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 8.862003492149384e-11\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 2.9514066191005384e-10\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.6800904\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 1.1524871335044651e-10\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 1.0886153090083894e-10\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 1.83281598231666e-10\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 2.6905072614269443e-10\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.68191606\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 5.3370235314487147e-11\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 5.47136329298592e-11\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 3.3253325093518527e-10\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 2.97909918955952e-10\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.71716285\n",
      "\n",
      "0.75 1\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.2782374620437622\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.01640581339597702\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.016626114025712013\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.016342835500836372\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.016153287142515182\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.016333183273673058\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.01638425700366497\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.01651105284690857\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.01622350886464119\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.01632769964635372\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.016290675848722458\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.016364289447665215\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.016279790550470352\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.01641744002699852\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.01640232466161251\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.016388580203056335\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.016139447689056396\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.016228463500738144\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.01638820208609104\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.016377633437514305\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.016071587800979614\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.016266729682683945\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.016304360702633858\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.016391389071941376\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.016056079417467117\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.016243087127804756\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.016136765480041504\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.01629907824099064\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.015979478135704994\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.016253065317869186\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.01618942618370056\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.01633683405816555\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.01582428440451622\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.0161804910749197\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.01611756533384323\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.016242967918515205\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.015790387988090515\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.01615760661661625\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.016028715297579765\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.016271322965621948\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.75 2\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.10009631514549255\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.00978176575154066\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.009920064359903336\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.009954660199582577\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.009697888046503067\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.00968144554644823\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.0097317760810256\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.009827208705246449\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.009622114710509777\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.009763932786881924\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.009844156913459301\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.009774686768651009\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.009590361267328262\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.009774403646588326\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.00965175125747919\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.009781703352928162\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.009596719406545162\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.00964454747736454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.009740148670971394\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.00971137173473835\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.00956396572291851\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.00966781098395586\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.009646846912801266\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.009775400161743164\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.009432363323867321\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.00961439311504364\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.009638329967856407\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.00974886491894722\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.009409024380147457\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.009684634394943714\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.00961338821798563\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.009733595885336399\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.009287755005061626\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.009608713909983635\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.00952430535107851\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.009648635983467102\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.009337738156318665\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.009489418938755989\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.009530140087008476\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.009691904298961163\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.75 5\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.010046407580375671\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.0018321927636861801\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.0018609571270644665\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.0018574954010546207\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.0017146157333627343\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.0016975243343040347\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.0017001223750412464\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.0016665398143231869\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.0016470163827762008\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.0016655996441841125\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.0016905268421396613\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.0016474288422614336\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.0016666953451931477\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.0016590122831985354\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.0016624429263174534\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.0016763907624408603\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.0016361514572054148\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.001671323087066412\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.001657511806115508\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.001657619490288198\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.0016200337558984756\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.0016332981176674366\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.0016532351728528738\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.0016530376160517335\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.0016222273698076606\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.0016390960663557053\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.0016583582619205117\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.0016629573656246066\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.0016094879247248173\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.0016463762149214745\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.001638875575736165\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.0016466747038066387\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.0016166972927749157\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.0016377633437514305\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.001628698082640767\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.001658637193031609\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.0015969699015840888\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.001664714072830975\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.0016524011734873056\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.0016773935640230775\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.75 10\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.022262470796704292\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.0001040240895235911\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.00010966693662339821\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.00010168547305511311\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.9765213\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 9.592077549314126e-05\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 8.655904821353033e-05\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 8.412172610405833e-05\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 8.400846127187833e-05\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97964627\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 8.261158654931933e-05\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 7.813114643795416e-05\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 7.815536082489416e-05\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 7.819922757335007e-05\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.9799752\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 7.521224324591458e-05\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 6.806338205933571e-05\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 7.904334052000195e-05\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 6.99192241881974e-05\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 7.375550194410607e-05\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 6.951448449399322e-05\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 6.855305400677025e-05\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 6.544929055962712e-05\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 6.689490692224354e-05\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 6.611912976950407e-05\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 6.506263162009418e-05\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 6.554069113917649e-05\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 6.475565169239417e-05\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 6.438698619604111e-05\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 6.555057188961655e-05\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 6.656438199570403e-05\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 6.50565852993168e-05\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 6.634209421463311e-05\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 6.425855826819316e-05\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 6.458108691731468e-05\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 6.527634832309559e-05\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 6.384098378475755e-05\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 6.333836790872738e-05\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 6.391321949195117e-05\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 6.320118700386956e-05\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 6.320798274828121e-05\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 6.461184966610745e-05\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 6.390073394868523e-05\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.75 20\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 6.27981498837471e-05\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 1.542495397188759e-06\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 7.113605988706695e-07\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 4.551183394596592e-07\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.89398843\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 6.276432031881995e-07\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 5.7747172377276e-07\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 3.74804500324899e-07\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 2.7924841106141685e-07\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.9191777\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 3.1813439704819757e-07\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 2.396796503489895e-07\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 2.2290896595222875e-07\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 3.186387687037495e-07\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.94786185\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 2.387010908933007e-07\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 1.9695083608439745e-07\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 2.1772873992631503e-07\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 1.8380414701368863e-07\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.9612746\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 1.722099653989062e-07\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 1.6933182678258163e-07\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 1.4435495643283502e-07\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 1.6519872758635756e-07\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.963339\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 1.2889624656509113e-07\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 1.5459215774171753e-07\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 1.5406959619213012e-07\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 1.4342211329676502e-07\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.9656498\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 1.2334400878444285e-07\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 1.382404946070892e-07\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 1.299299157153655e-07\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 1.211874689488468e-07\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.9719737\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 1.2844047603266517e-07\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 1.317300757364137e-07\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 1.1981933312199544e-07\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 1.3001572085613589e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97631586\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 1.1333272453839527e-07\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 1.120401051935005e-07\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 1.0554877860613487e-07\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 1.0808712147536426e-07\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.9766119\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 1.0947848494424761e-07\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 1.0513817727542119e-07\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 9.991135385689631e-08\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 1.063578736193449e-07\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.9774425\n",
      "\n",
      "0.75 50\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.0006129413959570229\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 6.289033877004968e-11\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 1.5916322426701157e-10\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 2.15200746112032e-10\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.95142275\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 1.2359568923869801e-10\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 3.9979092258946025e-10\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 6.137069741285472e-10\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 7.509217136991708e-10\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.94669414\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 2.633439646904101e-11\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 4.311539458790037e-10\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 6.405386637764465e-11\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 2.3363036505408274e-10\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.9388733\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 4.628591607214183e-11\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 5.239595909922734e-11\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 4.2475065131775125e-10\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 6.939292456625834e-12\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.92731094\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 1.3502791766650812e-11\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 4.6629228256378497e-11\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 1.0916269971295023e-10\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 7.631215104275668e-12\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.90953124\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 8.325746342352502e-12\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 1.1501654663403915e-11\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 2.780643215682943e-10\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 6.295386954791038e-12\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.8971628\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 1.393407195182661e-10\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 6.077565534168272e-11\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 9.920941845820153e-11\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 4.468297606918803e-11\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.8865132\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 1.6816617542936285e-11\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 1.8708099305819736e-11\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 1.8242803101475857e-10\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 1.0085095085432538e-11\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.8761513\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 3.207560259066433e-11\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 3.011301763500285e-10\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 8.095595374624232e-12\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 1.712371043571803e-11\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.8567517\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 1.3418599564829492e-11\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 5.34044232447517e-11\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 7.351854368342625e-12\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 7.537361360054895e-12\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.8480838\n",
      "\n",
      "0.9 1\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.5893113017082214\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.00814680103212595\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.007890420965850353\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.007871060632169247\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.007796030957251787\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.007778657600283623\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.007823021151125431\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.0078273368999362\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.007827816531062126\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.007724760565906763\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.007788612507283688\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.0078027513809502125\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.007774920668452978\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.007801818661391735\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.007765382993966341\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.007813570089638233\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.007695209234952927\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.007749560754746199\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.007811014074832201\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.007828092202544212\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.007657981943339109\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.00773105351254344\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.00777539424598217\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.007821005769073963\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.007713709957897663\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.007720509544014931\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.007762426510453224\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.0078046186827123165\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.007648928090929985\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.007743333466351032\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.007731919642537832\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.007796290796250105\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.00761315505951643\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.007723881863057613\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.007769265212118626\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.007798012346029282\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.007634410634636879\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.0077070617116987705\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.0077226958237588406\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.007806718349456787\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.9 2\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.13837581872940063\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.004871229641139507\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.004895334597676992\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.004926182329654694\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.0049946848303079605\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.004919441416859627\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.0048895226791501045\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.004913921933621168\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.00483216205611825\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.004816535394638777\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.004800646100193262\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.00483150128275156\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.004858199041336775\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.0048187351785600185\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.004834604915231466\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.00484267296269536\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.004807848017662764\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.004812140017747879\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.004815723281353712\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.004818527027964592\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.00477920426055789\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.004795325454324484\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.004816887900233269\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.004847465083003044\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.004764985758811235\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.004824843257665634\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.004828695673495531\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.004844641778618097\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.0047324905171990395\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.004785408731549978\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.004846770316362381\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.004836708772927523\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.00477003725245595\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.0048257592134177685\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.004815824795514345\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.004840568173676729\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.004733072593808174\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.004786549136042595\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.004785171244293451\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.004843265749514103\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.9 5\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.04245270788669586\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.001134391874074936\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.0010202069533988833\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.0010020838817581534\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.000980348908342421\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.0009599108598195016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.0009677972993813455\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.0009546024375595152\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.0009586763917468488\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.000953031238168478\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.0009400275885127485\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.0009329316089861095\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.000929031812120229\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.0009357371018268168\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.0009505693451501429\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.0009408096084371209\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.0009264994878321886\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.0009194259764626622\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.000935243209823966\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.0009416836546733975\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.0009210589341819286\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.0009175076265819371\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.0009352236520498991\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.0009261590312235057\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.0009207851253449917\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.0009289379231631756\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.0009216293692588806\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.0009288040455430746\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.0009138858877122402\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.0009256931953132153\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.0009353762725368142\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.0009335328941233456\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.0009025945910252631\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.000910326954908669\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.0009286214481107891\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.00093816191656515\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.0009054887341335416\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.0009105799836106598\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.0009183491347357631\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.0009302934631705284\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.9 10\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.0024610802065581083\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 6.568168464582413e-05\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 5.958496331004426e-05\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 6.221033254405484e-05\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.979893\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 5.653524203808047e-05\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 4.6473163820337504e-05\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 4.695416282629594e-05\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 5.243706254987046e-05\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 4.4562148104887456e-05\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 4.210820407024585e-05\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 4.276904292055406e-05\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 4.500245631788857e-05\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 4.1021499782800674e-05\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 4.1463128582108766e-05\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 4.0421393350698054e-05\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 4.167320003034547e-05\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 3.97649964725133e-05\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 4.022652137791738e-05\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 3.918338188668713e-05\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 4.039482882944867e-05\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 4.040206113131717e-05\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 3.927120997104794e-05\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 3.976837615482509e-05\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 3.928678052034229e-05\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 3.859228309011087e-05\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 3.9252336136996746e-05\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 3.809364352491684e-05\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 4.039736450067721e-05\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 3.842883597826585e-05\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 3.86845858884044e-05\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 3.9863891288405284e-05\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 3.854339956887998e-05\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 3.842787555186078e-05\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 3.8760521420044824e-05\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 3.929320155293681e-05\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 3.862045196001418e-05\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97999996\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 3.901253148796968e-05\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 3.8941125239944085e-05\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 3.864290556521155e-05\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 3.812368959188461e-05\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.97999996\n",
      "\n",
      "0.9 20\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 4.043969602207653e-05\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 7.599807645419787e-07\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 6.5456515585538e-07\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 4.0785437249724055e-07\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.9331085\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 2.758516757239704e-07\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 2.1994955545778794e-07\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 2.1876948608223756e-07\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 2.533607244004088e-07\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.95965457\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 1.4145702209589217e-07\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 1.439277355075319e-07\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 1.3310794599874498e-07\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 1.2817275774068548e-07\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.96198183\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 1.2147110339810752e-07\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 1.0921155535470461e-07\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 1.0782792259078633e-07\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 1.001886076323899e-07\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.9731661\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 1.0643044845437544e-07\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 1.086028902363978e-07\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 8.620425262506615e-08\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 8.990641475747907e-08\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.97680914\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 8.326544076453501e-08\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 8.912707727404268e-08\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 9.669437872616982e-08\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 8.896930125956715e-08\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.9781415\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 7.516207034541367e-08\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 7.967431514543932e-08\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 8.404969520370287e-08\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 7.233952459273496e-08\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.97895545\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 7.14364531972933e-08\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 7.2105201809336e-08\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 7.303521698531767e-08\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 6.65056560933408e-08\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.97943234\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 7.825328651733798e-08\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 6.210500913539363e-08\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 7.425532544402813e-08\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 7.103857768697708e-08\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.97953105\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 6.791046303078474e-08\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 7.171295379748699e-08\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 6.155932652518459e-08\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 6.779639960541317e-08\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.9798519\n",
      "\n",
      "0.9 50\n",
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.00022630403691437095\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 4.031914357938149e-08\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 3.111667368216331e-08\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 6.824147358308608e-10\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.9627715\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 6.306407618339449e-10\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 4.7134567360274104e-09\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 4.6808557030431075e-11\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 3.5536826953341816e-11\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.93097043\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 4.3027181817478777e-10\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 1.096526341948234e-10\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 1.0760317081803805e-09\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 8.42001208339882e-12\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.8988486\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 3.1679169704146304e-11\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 1.4563786288057656e-10\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 4.027098446379718e-11\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 1.356743328795318e-10\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.86780435\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 8.511811394606994e-11\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 3.723037503289284e-11\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 1.0449323697980795e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 1.8041843019567239e-10\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.8444243\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 1.029386090256601e-10\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 1.676114802506845e-11\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 9.0619022702354e-12\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 8.706316917406198e-12\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.8176563\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 1.5013596268986795e-11\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 5.817860949941522e-12\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 9.286383294282441e-12\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 6.624714665726117e-11\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.79983544\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 1.5760380500662308e-10\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 6.521275186521791e-11\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 4.346452017744973e-11\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 1.2966300776129369e-11\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.7723108\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 6.749664716032555e-11\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 3.2446524633744644e-11\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 1.2938303206588841e-11\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 2.5786575999497963e-11\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.7605345\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 3.868019446406912e-11\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 2.7131186186579725e-11\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 1.2098244281388837e-11\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 6.763144298066459e-12\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.76548517\n"
     ]
    }
   ],
   "source": [
    "for a in [0.25, 0.5, 0.75, 0.9]:\n",
    "    for g in [1, 2, 5, 10, 20, 50]:\n",
    "        \n",
    "        print()\n",
    "        print(a, g)\n",
    "        model = ToyModel(VOCAB_SIZE)\n",
    "        model = model.cuda()\n",
    "        optimizer = RAdam(model.parameters(), lr=LR)\n",
    "        loss_fn = FocalLoss(gamma=g, alpha=a)\n",
    "        trn_loss, val_loss, val_acc = train(model, train_dl, valid_dl, loss_fn, optimizer, num_epochs=10, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot these results in a chart with alpha across the top and gamma across the side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|   gamma   | 0.25 | 0.50 | 0.75 | 0.90 |\n",
    "| --------- |:----:|:----:|:- --:|:----:|\n",
    "| 1.00      | 0.98 | 0.98 | 0.98 | 0.98 |\n",
    "| 2.00      | 0.98 | 0.98 | 0.98 | 0.98 |\n",
    "| 5.00      | 0.98 | 0.98 | 0.98 | 0.98 |\n",
    "| 10.0      | 0.98 | 0.98 | 0.98 | 0.98 |\n",
    "| 20.0      | 0.88 | 0.88 | 0.98 | 0.98 |\n",
    "| 50.0      | 0.63 | 0.72 | 0.85 | 0.77 |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, nothing seems to do that well. Let's investigate `gamma=20` and `alpha=0.25`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.25 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.00015222532965708524\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 1.2361989547571284e-06\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 7.249186069202551e-07\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 6.218057819751266e-07\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.7431414\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 3.820464087311848e-07\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 2.570633057530358e-07\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 3.08966264128685e-07\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 2.2850794323403534e-07\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.806949\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 2.8916059591210796e-07\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 2.0727487992644456e-07\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 2.124137949977012e-07\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 1.5737379044367117e-07\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.8666695\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 2.3050748154673784e-07\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 1.7288948583882302e-07\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 1.8109530230958626e-07\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 1.6186720586119918e-07\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.8027384\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 1.7014195918818587e-07\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 1.4581488017029187e-07\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 1.319685338785348e-07\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 1.630057084867076e-07\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.87902963\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 1.661275774722526e-07\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 1.1932475274534227e-07\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 1.4548329829722206e-07\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 1.2586247066792566e-07\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.89486843\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 1.5082811444244726e-07\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 1.2873529442458675e-07\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 1.2021149586871616e-07\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 1.3160749290364038e-07\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.9239555\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 1.2206993460495141e-07\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 1.291318767471239e-07\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 1.1064543770089585e-07\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 1.0628456692529653e-07\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.94018084\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 1.1011646705583189e-07\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 1.1288963719380263e-07\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 1.0644469483622743e-07\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 1.142494241435088e-07\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.93385684\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 1.00001734892885e-07\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 1.1692026902210273e-07\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 1.0626946789216163e-07\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 9.987204663275406e-08\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.947097\n",
      "Epoch:\t 10 \tStep:\t 0 \tLoss:\t 1.0098370495370546e-07\n",
      "Epoch:\t 10 \tStep:\t 100 \tLoss:\t 9.389939492621124e-08\n",
      "Epoch:\t 10 \tStep:\t 200 \tLoss:\t 1.1416153711252264e-07\n",
      "Epoch:\t 10 \tStep:\t 300 \tLoss:\t 9.862718286512973e-08\n",
      "Epoch:\t 10 \t\t\tValid Accuracy\t 0.9448355\n",
      "Epoch:\t 11 \tStep:\t 0 \tLoss:\t 1.107555007706651e-07\n",
      "Epoch:\t 11 \tStep:\t 100 \tLoss:\t 1.0108342252124203e-07\n",
      "Epoch:\t 11 \tStep:\t 200 \tLoss:\t 9.864688621519235e-08\n",
      "Epoch:\t 11 \tStep:\t 300 \tLoss:\t 9.7679780708404e-08\n",
      "Epoch:\t 11 \t\t\tValid Accuracy\t 0.955403\n",
      "Epoch:\t 12 \tStep:\t 0 \tLoss:\t 8.900678949430585e-08\n",
      "Epoch:\t 12 \tStep:\t 100 \tLoss:\t 9.916393395315026e-08\n",
      "Epoch:\t 12 \tStep:\t 200 \tLoss:\t 9.382423371562254e-08\n",
      "Epoch:\t 12 \tStep:\t 300 \tLoss:\t 1.0050007404061034e-07\n",
      "Epoch:\t 12 \t\t\tValid Accuracy\t 0.96575665\n",
      "Epoch:\t 13 \tStep:\t 0 \tLoss:\t 9.56789847350592e-08\n",
      "Epoch:\t 13 \tStep:\t 100 \tLoss:\t 9.694536373672236e-08\n",
      "Epoch:\t 13 \tStep:\t 200 \tLoss:\t 9.782529986068766e-08\n",
      "Epoch:\t 13 \tStep:\t 300 \tLoss:\t 1.011863588473716e-07\n",
      "Epoch:\t 13 \t\t\tValid Accuracy\t 0.9647286\n",
      "Epoch:\t 14 \tStep:\t 0 \tLoss:\t 9.52823953070947e-08\n",
      "Epoch:\t 14 \tStep:\t 100 \tLoss:\t 9.863602912218994e-08\n",
      "Epoch:\t 14 \tStep:\t 200 \tLoss:\t 9.07278945305734e-08\n",
      "Epoch:\t 14 \tStep:\t 300 \tLoss:\t 9.217111340831252e-08\n",
      "Epoch:\t 14 \t\t\tValid Accuracy\t 0.95889795\n",
      "Epoch:\t 15 \tStep:\t 0 \tLoss:\t 9.677174261923938e-08\n",
      "Epoch:\t 15 \tStep:\t 100 \tLoss:\t 9.238986820037098e-08\n",
      "Epoch:\t 15 \tStep:\t 200 \tLoss:\t 8.697775655264195e-08\n",
      "Epoch:\t 15 \tStep:\t 300 \tLoss:\t 8.978935284176259e-08\n",
      "Epoch:\t 15 \t\t\tValid Accuracy\t 0.972977\n",
      "Epoch:\t 16 \tStep:\t 0 \tLoss:\t 9.346491225414866e-08\n",
      "Epoch:\t 16 \tStep:\t 100 \tLoss:\t 8.590435385258388e-08\n",
      "Epoch:\t 16 \tStep:\t 200 \tLoss:\t 9.020768487744135e-08\n",
      "Epoch:\t 16 \tStep:\t 300 \tLoss:\t 8.762256697991688e-08\n",
      "Epoch:\t 16 \t\t\tValid Accuracy\t 0.97291946\n",
      "Epoch:\t 17 \tStep:\t 0 \tLoss:\t 8.539258544715267e-08\n",
      "Epoch:\t 17 \tStep:\t 100 \tLoss:\t 8.665944761787614e-08\n",
      "Epoch:\t 17 \tStep:\t 200 \tLoss:\t 8.8816420884541e-08\n",
      "Epoch:\t 17 \tStep:\t 300 \tLoss:\t 9.686429081057213e-08\n",
      "Epoch:\t 17 \t\t\tValid Accuracy\t 0.9751891\n",
      "Epoch:\t 18 \tStep:\t 0 \tLoss:\t 9.001236378480826e-08\n",
      "Epoch:\t 18 \tStep:\t 100 \tLoss:\t 9.158236480288906e-08\n",
      "Epoch:\t 18 \tStep:\t 200 \tLoss:\t 8.865983147643419e-08\n",
      "Epoch:\t 18 \tStep:\t 300 \tLoss:\t 9.10072230908554e-08\n",
      "Epoch:\t 18 \t\t\tValid Accuracy\t 0.97575647\n",
      "Epoch:\t 19 \tStep:\t 0 \tLoss:\t 9.064682160442317e-08\n",
      "Epoch:\t 19 \tStep:\t 100 \tLoss:\t 8.843132093261374e-08\n",
      "Epoch:\t 19 \tStep:\t 200 \tLoss:\t 8.723185374037712e-08\n",
      "Epoch:\t 19 \tStep:\t 300 \tLoss:\t 8.895614911352823e-08\n",
      "Epoch:\t 19 \t\t\tValid Accuracy\t 0.9779113\n"
     ]
    }
   ],
   "source": [
    "a = 0.25\n",
    "g = 20\n",
    "        \n",
    "print()\n",
    "print(a, g)\n",
    "model = ToyModel(VOCAB_SIZE)\n",
    "model = model.cuda()\n",
    "optimizer = RAdam(model.parameters(), lr=LR)\n",
    "loss_fn = FocalLoss(gamma=g, alpha=a)\n",
    "trn_loss, val_loss, val_acc = train(model, train_dl, valid_dl, loss_fn, optimizer, num_epochs=20, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\t tensor([866,  60, 960,   9, 750, 212,  37,  57, 561, 385,  99, 502, 205, 343,\n",
      "        119, 210, 210, 329, 302, 850, 325, 642, 382, 435, 859, 698, 288, 729,\n",
      "        652, 818, 300, 339, 255, 320,  23, 225, 199, 430, 941, 569, 367,  91,\n",
      "        231, 269, 538, 138, 164, 902, 356, 219, 901, 502, 639, 214, 650, 272,\n",
      "        762, 686, 776, 387, 529, 702, 457, 927, 941, 674, 119, 504, 651, 572,\n",
      "        321, 683, 417, 688, 426, 142, 699, 284, 198, 819, 813,  80,  94,  56,\n",
      "         75, 786, 234, 845, 529, 989, 520, 483, 281, 160, 199, 513, 751, 710,\n",
      "        813, 419], device='cuda:0')\n",
      "y:\t tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0')\n",
      "y_hat:\t tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Check if we're learned how to perform on items in the train dataset\n",
    "x, y = train_ds[7]\n",
    "y_hat = get_output_for_example(model, x)\n",
    "\n",
    "print(\"X:\\t\", x)\n",
    "print(\"y:\\t\", y)\n",
    "print(\"y_hat:\\t\", y_hat.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Train on short sequence, infer on full sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10x sequence length \n",
    "SEQ_LENGTH = 10\n",
    "# Other parameters are kept the same (as the working version)\n",
    "TRN_EXAMPLES = 12800\n",
    "VAL_EXAMPLES = 1200\n",
    "VOCAB_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "\n",
    "train_ds = ToyDataset(num_examples=TRN_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "valid_ds = ToyDataset(num_examples=VAL_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel(VOCAB_SIZE)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We use RAdam to avoid having to use warmup\n",
    "# If we use regular Adam, this usually won't converge for long sequences\n",
    "optimizer = RAdam(model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.6001947522163391\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.47228068113327026\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.4729190468788147\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.463817298412323\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.7999999\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.4583519995212555\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.46207141876220703\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.47874578833580017\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.45635169744491577\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.7999999\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.45821842551231384\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.4554305970668793\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.4782959818840027\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.45478326082229614\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.79999983\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.4555266499519348\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.44615983963012695\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.48448657989501953\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.44647520780563354\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.7890626\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.3732038736343384\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.2989511787891388\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.3330453038215637\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.14027056097984314\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.94695723\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.07185797393321991\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.04228468984365463\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.1962999403476715\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.07444138079881668\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97590446\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.034874796867370605\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.03374994173645973\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.06484569609165192\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.01878480613231659\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.96949005\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.01609973981976509\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.07024126499891281\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.08045993745326996\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.045140910893678665\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.98462176\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.0010030962293967605\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.0036620800383388996\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.13114191591739655\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.05276321619749069\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.9518914\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.09332562983036041\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.002131859539076686\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.020210683345794678\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.016419993713498116\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.9785363\n"
     ]
    }
   ],
   "source": [
    "trn_loss, val_loss, val_acc = train(model, train_dl, valid_dl, loss_fn, optimizer, num_epochs=10, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.5984412431716919\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.32517558336257935\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.21760711073875427\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.17405720055103302\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.93297696\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.22562703490257263\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.2391514778137207\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.15974754095077515\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.17933598160743713\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.9348273\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.1933424174785614\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.2186671495437622\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.15512634813785553\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.15357434749603271\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.9368832\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.1639295369386673\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.17978298664093018\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.1329062283039093\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.12996655702590942\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.94259876\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.14133837819099426\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.1494176834821701\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.10230495780706406\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.07789726555347443\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.9688734\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.08000429719686508\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.05296202376484871\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.046543922275304794\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.044067539274692535\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.97746706\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.07543032616376877\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.049181632697582245\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.07656531780958176\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.025255629792809486\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.9837582\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.030284881591796875\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.04117584973573685\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.04328572377562523\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.01289425604045391\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.98651314\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.0399787463247776\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.061198391020298004\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.023325299844145775\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.0031070925761014223\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.9877468\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.012763293460011482\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.019068608060479164\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.005568319000303745\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.0030548223294317722\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.9880757\n",
      "Epoch:\t 10 \tStep:\t 0 \tLoss:\t 0.023115109652280807\n",
      "Epoch:\t 10 \tStep:\t 100 \tLoss:\t 0.0056768180802464485\n",
      "Epoch:\t 10 \tStep:\t 200 \tLoss:\t 0.005913074128329754\n",
      "Epoch:\t 10 \tStep:\t 300 \tLoss:\t 0.0019845596980303526\n",
      "Epoch:\t 10 \t\t\tValid Accuracy\t 0.9897203\n",
      "Epoch:\t 11 \tStep:\t 0 \tLoss:\t 0.018773378804326057\n",
      "Epoch:\t 11 \tStep:\t 100 \tLoss:\t 0.006097109988331795\n",
      "Epoch:\t 11 \tStep:\t 200 \tLoss:\t 0.02055082842707634\n",
      "Epoch:\t 11 \tStep:\t 300 \tLoss:\t 0.004280916415154934\n",
      "Epoch:\t 11 \t\t\tValid Accuracy\t 0.99066615\n",
      "Epoch:\t 12 \tStep:\t 0 \tLoss:\t 0.011074991896748543\n",
      "Epoch:\t 12 \tStep:\t 100 \tLoss:\t 0.008204708807170391\n",
      "Epoch:\t 12 \tStep:\t 200 \tLoss:\t 0.008895866572856903\n",
      "Epoch:\t 12 \tStep:\t 300 \tLoss:\t 0.03846533223986626\n",
      "Epoch:\t 12 \t\t\tValid Accuracy\t 0.9914473\n",
      "Epoch:\t 13 \tStep:\t 0 \tLoss:\t 0.04473903402686119\n",
      "Epoch:\t 13 \tStep:\t 100 \tLoss:\t 0.012105138041079044\n",
      "Epoch:\t 13 \tStep:\t 200 \tLoss:\t 0.012846583500504494\n",
      "Epoch:\t 13 \tStep:\t 300 \tLoss:\t 0.0038510672748088837\n",
      "Epoch:\t 13 \t\t\tValid Accuracy\t 0.99058384\n",
      "Epoch:\t 14 \tStep:\t 0 \tLoss:\t 0.005313044413924217\n",
      "Epoch:\t 14 \tStep:\t 100 \tLoss:\t 0.0058652488514781\n",
      "Epoch:\t 14 \tStep:\t 200 \tLoss:\t 0.004689608700573444\n",
      "Epoch:\t 14 \tStep:\t 300 \tLoss:\t 0.0009037569398060441\n",
      "Epoch:\t 14 \t\t\tValid Accuracy\t 0.9907073\n"
     ]
    }
   ],
   "source": [
    "# longer sequence\n",
    "SEQ_LENGTH = 20\n",
    "LR = 1e-5\n",
    "\n",
    "train_ds = ToyDataset(num_examples=TRN_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "valid_ds = ToyDataset(num_examples=VAL_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)\n",
    "# NOTE: We use RAdam to avoid having to use warmup\n",
    "# If we use regular Adam, this usually won't converge for long sequences\n",
    "optimizer = RAdam(model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "trn_loss, val_loss, val_acc = train(model, train_dl, valid_dl, loss_fn, optimizer, num_epochs=15, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.3963231146335602\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.17041423916816711\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.12707240879535675\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.10694054514169693\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.96650916\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.14729033410549164\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.13385364413261414\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.11168750375509262\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.09593135863542557\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.9680305\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.12628930807113647\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.10003165155649185\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.08613862097263336\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.07808823883533478\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.9697779\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.10459296405315399\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.07464038580656052\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.09525598585605621\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.08132169395685196\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.97195727\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.09260611981153488\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.06779206544160843\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.07068613916635513\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.05427123233675957\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.9786389\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.053244806826114655\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.024484697729349136\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.02508106268942356\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.032290078699588776\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.9872739\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.023015284910798073\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.047666288912296295\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.012499747797846794\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.03381666913628578\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.99027544\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.008194453082978725\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.017997469753026962\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.015852726995944977\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.0110641960054636\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.99054265\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.007932750508189201\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.011026925407350063\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.010432063601911068\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.023150857537984848\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.9921463\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.00606332253664732\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.008161529898643494\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.014910625293850899\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.011616966687142849\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.992496\n",
      "Epoch:\t 10 \tStep:\t 0 \tLoss:\t 0.0068117170594632626\n",
      "Epoch:\t 10 \tStep:\t 100 \tLoss:\t 0.016210269182920456\n",
      "Epoch:\t 10 \tStep:\t 200 \tLoss:\t 0.002786864759400487\n",
      "Epoch:\t 10 \tStep:\t 300 \tLoss:\t 0.008481046184897423\n",
      "Epoch:\t 10 \t\t\tValid Accuracy\t 0.9922903\n",
      "Epoch:\t 11 \tStep:\t 0 \tLoss:\t 0.0012673622695729136\n",
      "Epoch:\t 11 \tStep:\t 100 \tLoss:\t 0.0031136744655668736\n",
      "Epoch:\t 11 \tStep:\t 200 \tLoss:\t 0.0067610605619847775\n",
      "Epoch:\t 11 \tStep:\t 300 \tLoss:\t 0.03800707310438156\n",
      "Epoch:\t 11 \t\t\tValid Accuracy\t 0.9942846\n",
      "Epoch:\t 12 \tStep:\t 0 \tLoss:\t 0.0030967399943619967\n",
      "Epoch:\t 12 \tStep:\t 100 \tLoss:\t 0.003348999423906207\n",
      "Epoch:\t 12 \tStep:\t 200 \tLoss:\t 0.01581648364663124\n",
      "Epoch:\t 12 \tStep:\t 300 \tLoss:\t 0.012184993363916874\n",
      "Epoch:\t 12 \t\t\tValid Accuracy\t 0.9939556\n",
      "Epoch:\t 13 \tStep:\t 0 \tLoss:\t 0.0012479211436584592\n",
      "Epoch:\t 13 \tStep:\t 100 \tLoss:\t 0.005386452190577984\n",
      "Epoch:\t 13 \tStep:\t 200 \tLoss:\t 0.008089023642241955\n",
      "Epoch:\t 13 \tStep:\t 300 \tLoss:\t 0.0035593039356172085\n",
      "Epoch:\t 13 \t\t\tValid Accuracy\t 0.9935445\n",
      "Epoch:\t 14 \tStep:\t 0 \tLoss:\t 0.000711682834662497\n",
      "Epoch:\t 14 \tStep:\t 100 \tLoss:\t 0.005113345570862293\n",
      "Epoch:\t 14 \tStep:\t 200 \tLoss:\t 0.003910894505679607\n",
      "Epoch:\t 14 \tStep:\t 300 \tLoss:\t 0.021562185138463974\n",
      "Epoch:\t 14 \t\t\tValid Accuracy\t 0.99362665\n"
     ]
    }
   ],
   "source": [
    "# longer sequence\n",
    "SEQ_LENGTH = 40\n",
    "LR = 1e-5\n",
    "\n",
    "train_ds = ToyDataset(num_examples=TRN_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "valid_ds = ToyDataset(num_examples=VAL_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)\n",
    "# NOTE: We use RAdam to avoid having to use warmup\n",
    "# If we use regular Adam, this usually won't converge for long sequences\n",
    "optimizer = RAdam(model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "trn_loss, val_loss, val_acc = train(model, train_dl, valid_dl, loss_fn, optimizer, num_epochs=15, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 0 \tStep:\t 0 \tLoss:\t 0.1583002805709839\n",
      "Epoch:\t 0 \tStep:\t 100 \tLoss:\t 0.06381018459796906\n",
      "Epoch:\t 0 \tStep:\t 200 \tLoss:\t 0.07213833183050156\n",
      "Epoch:\t 0 \tStep:\t 300 \tLoss:\t 0.06829557567834854\n",
      "Epoch:\t 0 \t\t\tValid Accuracy\t 0.9845232\n",
      "Epoch:\t 1 \tStep:\t 0 \tLoss:\t 0.0803603082895279\n",
      "Epoch:\t 1 \tStep:\t 100 \tLoss:\t 0.060652777552604675\n",
      "Epoch:\t 1 \tStep:\t 200 \tLoss:\t 0.06564901769161224\n",
      "Epoch:\t 1 \tStep:\t 300 \tLoss:\t 0.06427164375782013\n",
      "Epoch:\t 1 \t\t\tValid Accuracy\t 0.9848684\n",
      "Epoch:\t 2 \tStep:\t 0 \tLoss:\t 0.06996194273233414\n",
      "Epoch:\t 2 \tStep:\t 100 \tLoss:\t 0.055474553257226944\n",
      "Epoch:\t 2 \tStep:\t 200 \tLoss:\t 0.06913087517023087\n",
      "Epoch:\t 2 \tStep:\t 300 \tLoss:\t 0.060285214334726334\n",
      "Epoch:\t 2 \t\t\tValid Accuracy\t 0.98518914\n",
      "Epoch:\t 3 \tStep:\t 0 \tLoss:\t 0.07006382197141647\n",
      "Epoch:\t 3 \tStep:\t 100 \tLoss:\t 0.053473711013793945\n",
      "Epoch:\t 3 \tStep:\t 200 \tLoss:\t 0.060291942209005356\n",
      "Epoch:\t 3 \tStep:\t 300 \tLoss:\t 0.051710743457078934\n",
      "Epoch:\t 3 \t\t\tValid Accuracy\t 0.9848438\n",
      "Epoch:\t 4 \tStep:\t 0 \tLoss:\t 0.05442425608634949\n",
      "Epoch:\t 4 \tStep:\t 100 \tLoss:\t 0.036614399403333664\n",
      "Epoch:\t 4 \tStep:\t 200 \tLoss:\t 0.05381646752357483\n",
      "Epoch:\t 4 \tStep:\t 300 \tLoss:\t 0.04352862015366554\n",
      "Epoch:\t 4 \t\t\tValid Accuracy\t 0.986324\n",
      "Epoch:\t 5 \tStep:\t 0 \tLoss:\t 0.05454206466674805\n",
      "Epoch:\t 5 \tStep:\t 100 \tLoss:\t 0.018487250432372093\n",
      "Epoch:\t 5 \tStep:\t 200 \tLoss:\t 0.036183860152959824\n",
      "Epoch:\t 5 \tStep:\t 300 \tLoss:\t 0.02577345073223114\n",
      "Epoch:\t 5 \t\t\tValid Accuracy\t 0.989046\n",
      "Epoch:\t 6 \tStep:\t 0 \tLoss:\t 0.03585118427872658\n",
      "Epoch:\t 6 \tStep:\t 100 \tLoss:\t 0.025005221366882324\n",
      "Epoch:\t 6 \tStep:\t 200 \tLoss:\t 0.025256872177124023\n",
      "Epoch:\t 6 \tStep:\t 300 \tLoss:\t 0.024394739419221878\n",
      "Epoch:\t 6 \t\t\tValid Accuracy\t 0.9898272\n",
      "Epoch:\t 7 \tStep:\t 0 \tLoss:\t 0.03010289929807186\n",
      "Epoch:\t 7 \tStep:\t 100 \tLoss:\t 0.015446516685187817\n",
      "Epoch:\t 7 \tStep:\t 200 \tLoss:\t 0.025721799582242966\n",
      "Epoch:\t 7 \tStep:\t 300 \tLoss:\t 0.01847957819700241\n",
      "Epoch:\t 7 \t\t\tValid Accuracy\t 0.9904524\n",
      "Epoch:\t 8 \tStep:\t 0 \tLoss:\t 0.026373930275440216\n",
      "Epoch:\t 8 \tStep:\t 100 \tLoss:\t 0.01741006225347519\n",
      "Epoch:\t 8 \tStep:\t 200 \tLoss:\t 0.020005038008093834\n",
      "Epoch:\t 8 \tStep:\t 300 \tLoss:\t 0.013208123855292797\n",
      "Epoch:\t 8 \t\t\tValid Accuracy\t 0.9927055\n",
      "Epoch:\t 9 \tStep:\t 0 \tLoss:\t 0.013659090735018253\n",
      "Epoch:\t 9 \tStep:\t 100 \tLoss:\t 0.005099605303257704\n",
      "Epoch:\t 9 \tStep:\t 200 \tLoss:\t 0.009791497141122818\n",
      "Epoch:\t 9 \tStep:\t 300 \tLoss:\t 0.008534210734069347\n",
      "Epoch:\t 9 \t\t\tValid Accuracy\t 0.9925988\n",
      "Epoch:\t 10 \tStep:\t 0 \tLoss:\t 0.023876046761870384\n",
      "Epoch:\t 10 \tStep:\t 100 \tLoss:\t 0.0034637588541954756\n",
      "Epoch:\t 10 \tStep:\t 200 \tLoss:\t 0.009248808957636356\n",
      "Epoch:\t 10 \tStep:\t 300 \tLoss:\t 0.007930546067655087\n",
      "Epoch:\t 10 \t\t\tValid Accuracy\t 0.99251646\n",
      "Epoch:\t 11 \tStep:\t 0 \tLoss:\t 0.008093494921922684\n",
      "Epoch:\t 11 \tStep:\t 100 \tLoss:\t 0.01589411310851574\n",
      "Epoch:\t 11 \tStep:\t 200 \tLoss:\t 0.008693118579685688\n",
      "Epoch:\t 11 \tStep:\t 300 \tLoss:\t 0.017909104004502296\n",
      "Epoch:\t 11 \t\t\tValid Accuracy\t 0.99356914\n",
      "Epoch:\t 12 \tStep:\t 0 \tLoss:\t 0.015042279846966267\n",
      "Epoch:\t 12 \tStep:\t 100 \tLoss:\t 0.0036974826361984015\n",
      "Epoch:\t 12 \tStep:\t 200 \tLoss:\t 0.010146264918148518\n",
      "Epoch:\t 12 \tStep:\t 300 \tLoss:\t 0.002817431464791298\n",
      "Epoch:\t 12 \t\t\tValid Accuracy\t 0.99358547\n",
      "Epoch:\t 13 \tStep:\t 0 \tLoss:\t 0.006368415430188179\n",
      "Epoch:\t 13 \tStep:\t 100 \tLoss:\t 0.0032795059960335493\n",
      "Epoch:\t 13 \tStep:\t 200 \tLoss:\t 0.007289513945579529\n",
      "Epoch:\t 13 \tStep:\t 300 \tLoss:\t 0.00393596850335598\n",
      "Epoch:\t 13 \t\t\tValid Accuracy\t 0.99388975\n",
      "Epoch:\t 14 \tStep:\t 0 \tLoss:\t 0.005916259251534939\n",
      "Epoch:\t 14 \tStep:\t 100 \tLoss:\t 0.007623361889272928\n",
      "Epoch:\t 14 \tStep:\t 200 \tLoss:\t 0.0128706693649292\n",
      "Epoch:\t 14 \tStep:\t 300 \tLoss:\t 0.00907604955136776\n",
      "Epoch:\t 14 \t\t\tValid Accuracy\t 0.9944407\n"
     ]
    }
   ],
   "source": [
    "# longer sequence\n",
    "SEQ_LENGTH = 100\n",
    "LR = 1e-5\n",
    "\n",
    "train_ds = ToyDataset(num_examples=TRN_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "valid_ds = ToyDataset(num_examples=VAL_EXAMPLES, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)\n",
    "# NOTE: We use RAdam to avoid having to use warmup\n",
    "# If we use regular Adam, this usually won't converge for long sequences\n",
    "optimizer = RAdam(model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "trn_loss, val_loss, val_acc = train(model, train_dl, valid_dl, loss_fn, optimizer, num_epochs=15, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\t tensor([825, 162, 883, 218, 330, 571, 586, 169, 581, 856, 565, 257, 350, 955,\n",
      "        151, 466, 259, 895, 948, 656, 439, 336, 897,  22, 125, 692, 568, 565,\n",
      "        905, 323, 819, 796, 883, 235, 718, 746, 485, 113, 780, 768, 320, 683,\n",
      "        195, 670,  92, 329, 649, 233, 831, 804, 518, 660, 829, 107, 155, 715,\n",
      "        837, 303, 592, 548, 691, 644, 427, 408,  31, 587, 912, 722, 627, 338,\n",
      "        381, 515, 762, 976,  55, 334, 687, 687, 902, 233, 735, 795, 666, 994,\n",
      "        363, 640, 992, 803, 743, 431, 555, 854, 333, 918, 564, 926, 603, 749,\n",
      "        948, 223], device='cuda:0')\n",
      "y:\t tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0')\n",
      "y_hat:\t tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Check if we're learned how to perform on items in the train dataset\n",
    "x, y = valid_ds[0]\n",
    "y_hat = get_output_for_example(model, x)\n",
    "\n",
    "print(\"X:\\t\", x)\n",
    "print(\"y:\\t\", y)\n",
    "print(\"y_hat:\\t\", y_hat.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this approach seems to work, but I'm still left feeling a little pessimistic about it. I can imagine problems where we cannot subdivide them into smaller sequences and then scale up the sequence length.\n",
    "\n",
    "We still need a general solution.\n",
    "\n",
    "One good thing is that we now know that there definitely exists some set of weights that allow a Transformer to solve this problem. The hard part seems to be getting our model to learn these weights directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
