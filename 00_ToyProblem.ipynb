{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Problem 1\n",
    "\n",
    "\n",
    "I can't get my Transformer to work! Let's make the problem even simpler.\n",
    "\n",
    "Given a sequence of numbers, simply reverse the sequence.\n",
    "\n",
    "```\n",
    "input = 0 1 5 9 0 3 5 2 5\n",
    "reversed = 5 2 5 3 0 9 5 1 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "from radam import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_examples: int, seq_len: int, vocab_size: int):\n",
    "    inputs = np.random.randint(0, vocab_size, size=(num_examples, seq_len))\n",
    "    outputs = np.ascontiguousarray(np.flip(inputs, 1)) #PyTorch can't handle negative strides\n",
    "\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 6, 3, 7, 7, 5, 8, 2, 4, 2]]),\n",
       " array([[2, 4, 2, 8, 5, 7, 7, 3, 6, 2]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_data(1, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "   \n",
    "    def __init__(self, num_examples, sequence_length, vocab_size):\n",
    "        self.items, self.labels = generate_data(num_examples, sequence_length, vocab_size)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = torch.Tensor(self.items[idx]).long()\n",
    "        y = torch.Tensor(self.labels[idx]).long()\n",
    "        return x.cuda(), y.cuda()\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 5\n",
    "SEQUENCE_LENGTH = 100\n",
    "VOCAB_SIZE = 10\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ToyDataset(num_examples=NUM_EXAMPLES, sequence_length=SEQUENCE_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around a BERT model that predicts a single binary output for each input token\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Untrained BERT Model\n",
    "        config = BertConfig(vocab_size_or_config_json_file=vocab_size)\n",
    "        self.bert_model = BertModel(config)\n",
    "        self.linear = torch.nn.Linear(in_features=768, out_features=vocab_size)\n",
    "        \n",
    "    def forward(self, x):    \n",
    "        out, _ = self.bert_model(x)\n",
    "        out = self.linear(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel(VOCAB_SIZE)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We use RAdam to avoid having to use warmup\n",
    "# If we use regular Adam, this usually won't converge for long sequences\n",
    "optimizer = RAdam(model.parameters(), lr=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 tensor(1.8428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1 0 tensor(1.8082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2 0 tensor(1.7703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3 0 tensor(1.7303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4 0 tensor(1.7028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5 0 tensor(1.6839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6 0 tensor(1.6357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7 0 tensor(1.6156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8 0 tensor(1.5824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9 0 tensor(1.5314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "10 0 tensor(1.4967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "11 0 tensor(1.4975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "12 0 tensor(1.4305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "13 0 tensor(1.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "14 0 tensor(1.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "15 0 tensor(1.3393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "16 0 tensor(1.2808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "17 0 tensor(1.2689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "18 0 tensor(1.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "19 0 tensor(1.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "20 0 tensor(1.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "21 0 tensor(1.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "22 0 tensor(1.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "23 0 tensor(1.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "24 0 tensor(1.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "25 0 tensor(1.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "26 0 tensor(0.9802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "27 0 tensor(0.9542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "28 0 tensor(0.9369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "29 0 tensor(0.8924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "30 0 tensor(0.8775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "31 0 tensor(0.8431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "32 0 tensor(0.8006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "33 0 tensor(0.7946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "34 0 tensor(0.7543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "35 0 tensor(0.7134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "36 0 tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "37 0 tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "38 0 tensor(0.6562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "39 0 tensor(0.6488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "40 0 tensor(0.5870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "41 0 tensor(0.6152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "42 0 tensor(0.6008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "43 0 tensor(0.5830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "44 0 tensor(0.5504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "45 0 tensor(0.5308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "46 0 tensor(0.4956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "47 0 tensor(0.4752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "48 0 tensor(0.4660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "49 0 tensor(0.4550, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    \n",
    "    for train_step, (x, y) in enumerate(train_dl):\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output = model(x)\n",
    "        \n",
    "        loss = loss_fn(output.view((-1, output.size(-1))), y.view(-1))\n",
    "        \n",
    "        #print(output[0][:10])\n",
    "        if train_step % 100 == 0:\n",
    "            print(i, train_step, loss)\n",
    "        \n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_for_example(x):\n",
    "    \n",
    "    logits = model(x.unsqueeze(0))\n",
    "    probs = torch.softmax(logits, dim=2)\n",
    "    out = torch.argmax(probs, 2)\n",
    "    \n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\t tensor([6, 3, 5, 3, 0, 2, 2, 0, 6, 7, 1, 8, 4, 0, 4, 4, 2, 4, 1, 3, 0, 9, 6, 8,\n",
      "        1, 7, 2, 3, 2, 4, 0, 2, 3, 3, 9, 1, 5, 9, 1, 3, 8, 7, 5, 1, 5, 7, 2, 3,\n",
      "        7, 6, 1, 5, 0, 7, 4, 7, 7, 1, 5, 5, 0, 6, 7, 4, 0, 9, 4, 4, 5, 7, 9, 8,\n",
      "        1, 6, 1, 8, 1, 0, 4, 1, 0, 3, 1, 2, 7, 1, 0, 9, 1, 1, 5, 0, 9, 8, 5, 2,\n",
      "        6, 3, 0, 0], device='cuda:0')\n",
      "y:\t tensor([0, 0, 3, 6, 2, 5, 8, 9, 0, 5, 1, 1, 9, 0, 1, 7, 2, 1, 3, 0, 1, 4, 0, 1,\n",
      "        8, 1, 6, 1, 8, 9, 7, 5, 4, 4, 9, 0, 4, 7, 6, 0, 5, 5, 1, 7, 7, 4, 7, 0,\n",
      "        5, 1, 6, 7, 3, 2, 7, 5, 1, 5, 7, 8, 3, 1, 9, 5, 1, 9, 3, 3, 2, 0, 4, 2,\n",
      "        3, 2, 7, 1, 8, 6, 9, 0, 3, 1, 4, 2, 4, 4, 0, 4, 8, 1, 7, 6, 0, 2, 2, 0,\n",
      "        3, 5, 3, 6], device='cuda:0')\n",
      "y_hat:\t tensor([0, 0, 3, 6, 2, 5, 5, 9, 0, 5, 1, 1, 9, 0, 1, 7, 2, 1, 3, 4, 1, 4, 0, 5,\n",
      "        8, 3, 5, 1, 0, 9, 1, 5, 4, 4, 9, 9, 4, 7, 6, 0, 5, 5, 1, 7, 7, 4, 7, 0,\n",
      "        2, 1, 6, 7, 3, 2, 7, 5, 7, 5, 7, 8, 3, 0, 9, 5, 1, 7, 3, 3, 2, 5, 4, 2,\n",
      "        3, 2, 7, 1, 8, 6, 9, 0, 3, 1, 0, 2, 4, 9, 0, 4, 8, 1, 7, 6, 0, 2, 2, 9,\n",
      "        3, 5, 3, 6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y = train_ds[0]\n",
    "y_hat = get_output_for_example(x)\n",
    "\n",
    "print(\"X:\\t\", x)\n",
    "print(\"y:\\t\", y)\n",
    "print(\"y_hat:\\t\", y_hat.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(np.arange(SEQUENCE_LENGTH)).long().cuda()\n",
    "y = torch.flip(x, dims=(0,))\n",
    "y_hat = get_output_for_example(x)\n",
    "\n",
    "print(\"X:\\t\", x)\n",
    "print(\"y:\\t\", y)\n",
    "print(\"y_hat:\\t\", y_hat.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the first problem mostly working, let's try another.\n",
    "\n",
    "\n",
    "Given a sequence of numbers, find and mark adjacent duplicates. For simplicity, we'll only have one duplicate in each input.\n",
    "\n",
    "```\n",
    "input =    0 3 5 9 3 3 5 2 5\n",
    "reversed = 0 0 0 0 1 1 0 0 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_examples: int, seq_len: int, vocab_size: int):\n",
    "    inputs = np.random.randint(0, vocab_size, size=(num_examples, seq_len))\n",
    "    outputs = np.zeros_like(inputs)\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "        \n",
    "        # choose random location to introduce duplicate\n",
    "        location = np.random.randint(0, len(inputs[i]))\n",
    "        \n",
    "        left_or_right = np.random.random()\n",
    "        if left_or_right < 0.5 and location > 0:\n",
    "            dup_location = location - 1\n",
    "        elif left_or_right > 0.5 and location < len(inputs[i]) - 1:\n",
    "            dup_location = location + 1\n",
    "        elif location == 0:\n",
    "            dup_location = location + 1\n",
    "        elif location == len(inputs[i]) - 1:\n",
    "            dup_location = location - 1\n",
    "        else:\n",
    "            print(\"location\", location)\n",
    "            print(\"len of inputs[i]\", len(inputs[i]))\n",
    "            print(\"left or right\", left_or_right)\n",
    "            raise Exception(\"This should be unreachable...\")\n",
    "            \n",
    "        inputs[i][location] = inputs[i][dup_location]\n",
    "        \n",
    "        # Mark location where duplicates exist\n",
    "        outputs[i][location] = 1\n",
    "        outputs[i][dup_location] = 1\n",
    "\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[235, 247, 514, 522, 164, 540, 613, 985, 450, 450]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're lazy so we're going to just use a large vocabulary \n",
    "# instead of manually removing accidental duplicates\n",
    "generate_data(num_examples=1, seq_len=10, vocab_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "   \n",
    "    def __init__(self, num_examples, sequence_length, vocab_size):\n",
    "        self.items, self.labels = generate_data(num_examples, sequence_length, vocab_size)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = torch.Tensor(self.items[idx]).long()\n",
    "        y = torch.Tensor(self.labels[idx]).long()\n",
    "        return x.cuda(), y.cuda()\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 10000\n",
    "SEQUENCE_LENGTH = 10\n",
    "VOCAB_SIZE = 1000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_ds = ToyDataset(num_examples=NUM_EXAMPLES, sequence_length=SEQUENCE_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[817, 831, 314, 331, 855, 339, 339, 176, 606, 130],\n",
       "         [204, 233, 471, 471, 538, 475, 355, 958, 378, 967],\n",
       "         [930, 963, 420, 420, 950, 947, 934, 835, 983, 151],\n",
       "         [291, 991, 321, 242, 160, 625, 251, 251, 539,  50],\n",
       "         [725, 887,  87, 837, 953, 472, 816, 671, 671, 929],\n",
       "         [ 17,  50, 168, 168, 340, 673, 146, 862, 795, 119],\n",
       "         [213, 858, 858, 999, 972, 158, 906, 913, 924, 307],\n",
       "         [298, 816, 750, 750, 973, 564, 975, 400, 978, 948],\n",
       "         [450, 311, 354, 285, 140, 267, 267, 877, 693, 644],\n",
       "         [816, 964, 394, 184, 286, 286, 196, 932, 327, 579]], device='cuda:0'),\n",
       " tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "         [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       "         [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 1, 0, 0, 0, 0]], device='cuda:0'))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around a BERT model that predicts a single binary output for each input token\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Untrained BERT Model\n",
    "        config = BertConfig(vocab_size_or_config_json_file=vocab_size)\n",
    "        self.bert_model = BertModel(config)\n",
    "        self.linear = torch.nn.Linear(in_features=768, out_features=2)\n",
    "        \n",
    "    def forward(self, x):    \n",
    "        out, _ = self.bert_model(x)\n",
    "        out = self.linear(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel(VOCAB_SIZE)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We use RAdam to avoid having to use warmup\n",
    "# If we use regular Adam, this usually won't converge for long sequences\n",
    "optimizer = RAdam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "0 100 tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1 0 tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1 100 tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2 0 tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2 100 tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3 0 tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3 100 tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4 0 tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4 100 tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5 0 tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5 100 tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6 0 tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6 100 tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7 0 tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7 100 tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8 0 tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8 100 tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9 0 tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9 100 tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    \n",
    "    for train_step, (x, y) in enumerate(train_dl):\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output = model(x)\n",
    "        \n",
    "        loss = loss_fn(output.view((-1, output.size(-1))), y.view(-1))\n",
    "        \n",
    "        #print(output[0][:10])\n",
    "        if train_step % 100 == 0:\n",
    "            print(i, train_step, loss)\n",
    "               \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_for_example(x):\n",
    "    \n",
    "    logits = model(x.unsqueeze(0))\n",
    "    probs = torch.softmax(logits, dim=2)\n",
    "    out = torch.argmax(probs, 2)\n",
    "    \n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\t tensor([817, 831, 314, 331, 855, 339, 339, 176, 606, 130], device='cuda:0')\n",
      "y:\t tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0], device='cuda:0')\n",
      "y_hat:\t tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y = train_ds[0]\n",
    "y_hat = get_output_for_example(x)\n",
    "\n",
    "print(\"X:\\t\", x)\n",
    "print(\"y:\\t\", y)\n",
    "print(\"y_hat:\\t\", y_hat.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\t tensor([  0,   0, 200, 300, 400, 500, 600, 700, 800, 900], device='cuda:0')\n",
      "y:\t tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "y_hat:\t tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.arange(SEQUENCE_LENGTH)).long().cuda() * 100\n",
    "x[1] = 0\n",
    "y = torch.zeros_like(x)\n",
    "y[0] = 1\n",
    "y[1] = 1\n",
    "y_hat = get_output_for_example(x)\n",
    "\n",
    "print(\"X:\\t\", x)\n",
    "print(\"y:\\t\", y)\n",
    "print(\"y_hat:\\t\", y_hat.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
