{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the first problem mostly working, let's try another.\n",
    "\n",
    "\n",
    "Given a sequence of numbers, find and mark adjacent duplicates. Duplicates can be of length 2-5.\n",
    "\n",
    "```\n",
    "input  = 0 3 5 9 3 3 5 2 5\n",
    "output = 0 0 0 0 1 1 0 0 0\n",
    "\n",
    "input  = 1 2 2 2 2 2 3 1 5\n",
    "output = 0 1 1 1 1 1 0 0 0\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pessimisticdef generate_data(num_examples: int, seq_len: int, vocab_size: int):\n",
    "    inputs = np.random.randint(0, vocab_size, size=(num_examples, seq_len))\n",
    "    outputs = np.zeros_like(inputs)\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "        \n",
    "        # choose number of duplicates to introduce\n",
    "        num_duplicates = np.random.randint(3, 10)\n",
    "        \n",
    "        # choose random location to introduce our duplicates\n",
    "        location = np.random.randint(0, len(inputs[i]) - num_duplicates)\n",
    "        \n",
    "        # Choose what number we'd like to repeat\n",
    "        number_to_repeat = np.random.randint(0, vocab_size)\n",
    "        \n",
    "        inputs[i][location:location + num_duplicates] = number_to_repeat\n",
    "        \n",
    "        # Mark location where duplicates exist\n",
    "        outputs[i][location:location + num_duplicates] = 1\n",
    "        \n",
    "\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[668, 750, 750, 750, 750, 750, 256, 436, 632, 263]]),\n",
       " array([[0, 1, 1, 1, 1, 1, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're lazy so we're going to just use a large vocabulary \n",
    "# instead of manually removing accidental duplicates\n",
    "generate_data(num_examples=1, seq_len=10, vocab_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "   \n",
    "    def __init__(self, num_examples, sequence_length, vocab_size):\n",
    "        self.items, self.labels = generate_data(num_examples, sequence_length, vocab_size)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = torch.Tensor(self.items[idx]).long()\n",
    "        y = torch.Tensor(self.labels[idx]).long()\n",
    "        return x.cuda(), y.cuda()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 10000\n",
    "SEQUENCE_LENGTH = 10\n",
    "VOCAB_SIZE = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_ds = ToyDataset(num_examples=NUM_EXAMPLES, sequence_length=SEQUENCE_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around a BERT model that predicts a single binary output for each input token\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Untrained BERT Model\n",
    "        config = BertConfig(vocab_size_or_config_json_file=vocab_size)\n",
    "        self.bert_model = BertModel(config)\n",
    "        self.linear = torch.nn.Linear(in_features=768, out_features=2)\n",
    "        \n",
    "    def forward(self, x):    \n",
    "        out, _ = self.bert_model(x)\n",
    "        out = self.linear(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel(VOCAB_SIZE)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We use RAdam to avoid having to use warmup\n",
    "# If we use regular Adam, this usually won't converge for long sequences\n",
    "optimizer = RAdam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 tensor(0.8194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "0 100 tensor(0.4995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1 0 tensor(0.4675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1 100 tensor(0.4274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2 0 tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2 100 tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3 0 tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3 100 tensor(0.2139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4 0 tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4 100 tensor(0.1420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5 0 tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5 100 tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6 0 tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6 100 tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7 0 tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7 100 tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8 0 tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8 100 tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9 0 tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9 100 tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "10 0 tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "10 100 tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "11 0 tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "11 100 tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "12 0 tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "12 100 tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "13 0 tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "13 100 tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "14 0 tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "14 100 tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "15 0 tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "15 100 tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "16 0 tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "16 100 tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "17 0 tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "17 100 tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "18 0 tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "18 100 tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "19 0 tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "19 100 tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "20 0 tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "20 100 tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "21 0 tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "21 100 tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "22 0 tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "22 100 tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "23 0 tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "23 100 tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "24 0 tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "24 100 tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "25 0 tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "25 100 tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "26 0 tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "26 100 tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "27 0 tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "27 100 tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "28 0 tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "28 100 tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "29 0 tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "29 100 tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "30 0 tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "30 100 tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "31 0 tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "31 100 tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "32 0 tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "32 100 tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "33 0 tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "33 100 tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "34 0 tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "34 100 tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 35\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    \n",
    "    for train_step, (x, y) in enumerate(train_dl):\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output = model(x)\n",
    "        \n",
    "        loss = loss_fn(output.view((-1, output.size(-1))), y.view(-1))\n",
    "        \n",
    "        #print(output[0][:10])\n",
    "        if train_step % 100 == 0:\n",
    "            print(i, train_step, loss)\n",
    "               \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_for_example(x):\n",
    "    \n",
    "    logits = model(x.unsqueeze(0))\n",
    "    probs = torch.softmax(logits, dim=2)\n",
    "    out = torch.argmax(probs, 2)\n",
    "    \n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\t tensor([205, 456, 961, 961, 961, 961, 749, 574, 150, 492], device='cuda:0')\n",
      "y:\t tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:0')\n",
      "y_hat:\t tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Check if we're learned how to perform on items in the train dataset\n",
    "x, y = train_ds[0]\n",
    "y_hat = get_output_for_example(x)\n",
    "\n",
    "print(\"X:\\t\", x)\n",
    "print(\"y:\\t\", y)\n",
    "print(\"y_hat:\\t\", y_hat.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\t tensor([  9,   1, 222,  50, 250, 500, 600, 700, 800, 800], device='cuda:0')\n",
      "y:\t tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "y_hat:\t tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Check if we've learned how to perform on items out of sample\n",
    "x = torch.from_numpy(np.arange(SEQUENCE_LENGTH)).long().cuda() * 100\n",
    "x[0] = 9\n",
    "x[1] = 1\n",
    "x[2] = 222\n",
    "x[3] = 50\n",
    "x[4] = 250\n",
    "x[8] = 800\n",
    "x[9] = 800\n",
    "y = torch.zeros_like(x)\n",
    "y[0] = 1\n",
    "y[1] = 1\n",
    "y[2] = 1\n",
    "y[3] = 0\n",
    "y[4] = 0\n",
    "y_hat = get_output_for_example(x)\n",
    "\n",
    "print(\"X:\\t\", x)\n",
    "print(\"y:\\t\", y)\n",
    "print(\"y_hat:\\t\", y_hat.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
