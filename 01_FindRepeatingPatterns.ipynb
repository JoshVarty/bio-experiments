{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Tandem Repeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some DNA sequences there are repeating patterns that are located directly next to one another. These are called [Tandem Repeats](https://en.wikipedia.org/wiki/Tandem_repeat) and are of some interest to biologists.\n",
    "\n",
    "\n",
    "`ACGTACGTAAACGTAAACGTAAACGTGTGTACACCCAAAGTCA`\n",
    "\n",
    "In the above sequence, `ACGTAA` is repeated three times. This might warrant a closer look?\n",
    "\n",
    "Let's create a model that can detect repeated patterns within these sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dataset by \n",
    " - Randomly generating sequences of 100 random base pairs \n",
    " - Replacing elements of them with repeated patterns of \n",
    "   - Length 5-20. \n",
    "   - Patterns may be repeated 2-4 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNA_BASE_LENGTH = 100\n",
    "NUM_EXAMPLES = 100\n",
    "\n",
    "train_seq = []\n",
    "train_idx = np.zeros((NUM_EXAMPLES, DNA_BASE_LENGTH), dtype=np.int)\n",
    "\n",
    "# Create 100 random sequences of DNA bases\n",
    "for i, _ in enumerate(range(NUM_EXAMPLES)):\n",
    "    \n",
    "    dna_read = ''.join([random.choice('ACGT') for n in range(DNA_BASE_LENGTH)])\n",
    "    \n",
    "    # Build the repeated sequence\n",
    "    repetition_length = random.randint(5, 20)\n",
    "    number_of_repeats = random.randint(2, 4)\n",
    "    repeated_sequence = ''.join([random.choice('ACGT') for n in range(repetition_length)]) * number_of_repeats\n",
    "      \n",
    "    # Choose where to place the repetition\n",
    "    repeated_location = random.randint(0, len(dna_read) - 1 - len(repeated_sequence))\n",
    "    \n",
    "    # Insert the string\n",
    "    new_dna_read = dna_read[0:repeated_location] + repeated_sequence + dna_read[repeated_location + len(repeated_sequence):]\n",
    "    \n",
    "    # Mark the indexes of the start of reptitions\n",
    "    for j in range(number_of_repeats):\n",
    "        train_idx[i, repeated_location + (j * repetition_length)] = 1\n",
    "   \n",
    "    train_seq.append(new_dna_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CGTCCCGAAGTTGGCAAATGTGAGGTCAAGACGACTCACGTGGGTTGACGCAAACTGCGCCGCAGCCTGGGGGCCTGGGGGCCTGGGGGTTTTGATCGTA'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Handle accidentally repeated sequences. (For example we might have \"CC\" at the beginning or end of our sequence by chance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2PreTrainedModel, GPT2Model\n",
    "from transformers import BertConfig, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TandemRepeatDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset that handles simple tokenization of our DNA base sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_seq, train_idx):\n",
    "        \n",
    "        self.mapping = {'A':0, 'C': 1, 'G': 2, 'T': 3}\n",
    "        self.train_seq = self.tokenize(train_seq)\n",
    "        self.train_idx = train_idx\n",
    "        \n",
    "    def tokenize(self, train_seq):\n",
    "        tokenized_seq = []\n",
    "        \n",
    "        for seq in train_seq:\n",
    "            token_seq = [self.mapping[char] for char in seq]\n",
    "            tokenized_seq.append(token_seq)\n",
    "       \n",
    "        return tokenized_seq\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = torch.Tensor(self.train_seq[idx]).long()\n",
    "        y = torch.Tensor(self.train_idx[idx]).long()\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TandemRepeatDataset(train_seq, train_idx)\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 Model\n",
    "#config = GPT2Config.from_pretrained('gpt2', cache_dir=None)\n",
    "#model = GPT2Model.from_pretrained('gpt2', from_tf=False, config=config, cache_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TandemRepeatModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around a BERT model that predicts a single binary output for each input token\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = BertConfig.from_pretrained('bert-base-uncased', cache_dir=None)\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', from_tf=False, config=self.config, cache_dir='.')\n",
    "        self.bert_model.resize_token_embeddings(new_num_tokens=4)\n",
    "\n",
    "        self.linear =  torch.nn.Linear(in_features=768, out_features=2)\n",
    "        \n",
    "    def forward(self, x):    \n",
    "        out, _ = self.bert_model(x)\n",
    "        out = self.linear(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Model\n",
    "model = TandemRepeatModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow our model to accept only 4 input words (A, C, G or T)\n",
    "# Allow our model to only output 2 possibilies (0 or 1)\n",
    "#model.lm_head = torch.nn.Linear(in_features=768, out_features=2, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 1\n",
    "\n",
    "# for i in range(NUM_EPOCHS):\n",
    "    \n",
    "#     for x, y in train_dl:\n",
    "#         print(x,y)\n",
    "        \n",
    "#         model.zero_grad()\n",
    "#         loss = model(x, y)\n",
    "        \n",
    "#         loss.backward()\n",
    "        \n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
